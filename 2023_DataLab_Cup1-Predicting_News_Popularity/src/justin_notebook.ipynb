{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>DataLab Cup 1: Text Feature Engineering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. To load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../dataset/train.csv')\n",
    "test_data  = pd.read_csv('../dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11847, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
       "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
       "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
       "4   4          -1  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27643</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27644</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27645</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27646</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27647</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id                                       Page content\n",
       "0  27643  <html><head><div class=\"article-info\"><span cl...\n",
       "1  27644  <html><head><div class=\"article-info\"><span cl...\n",
       "2  27645  <html><head><div class=\"article-info\"><span cl...\n",
       "3  27646  <html><head><div class=\"article-info\"><span cl...\n",
       "4  27647  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "display(train_data.head())\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. To extract the features from the dataset\n",
    "\n",
    "將一些我們想要用到的feature從dataset中提取出來。以下為提取的特徵:\n",
    "\n",
    "- title\n",
    "- time(year/month/day/hour/minute/second)\n",
    "- number of images (num_img)\n",
    "- number of videos (num_video)\n",
    "- author name\n",
    "- topic\n",
    "- channel\n",
    "- length of content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "# to get the attribute of the 'title', 'year/month/date/day/hour/minute/second/is_weekend', 'num_img', 'num_video', 'author name', 'topic', 'channel', 'content length'\n",
    "\n",
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # 1. to find the 'title' (body > h1)\n",
    "    title = soup.find('h1').string.strip().lower()\n",
    "\n",
    "    # 2. to find time(body > div > span > time)\n",
    "\n",
    "    \n",
    "    date_string = soup.find('time')\n",
    "    try:\n",
    "        date_string = date_string['datetime']\n",
    "    except:\n",
    "        date_string = 'wed, 10 oct 2014 15:00:43 +0000'\n",
    "        \n",
    "    date_string = date_string.strip().lower()\n",
    "    datetimes = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S %z')\n",
    "    \n",
    "    \n",
    "    year = datetimes.year\n",
    "    month = datetimes.month\n",
    "    date = datetimes.day\n",
    "    day = pd.Timestamp(str(year)+'-'+str(month)+'-'+str(date)).dayofweek+1\n",
    "    is_weekend = 1 if (day==6 or day==7) else 0\n",
    "    hour = datetimes.hour\n",
    "    minute = datetimes.minute\n",
    "    second = datetimes.second\n",
    "    \n",
    "\n",
    "    # 3. to find the number of images\n",
    "    num_img  = len(soup.find_all('img'))\n",
    "    \n",
    "\n",
    "    # 4. to find the number of videos\n",
    "    num_video = len(soup.find_all('iframe'))\n",
    "    \n",
    "\n",
    "    # 5. to find the author name\n",
    "    article_info = soup.find('div', class_='article-info')\n",
    "    author = article_info.find('span', class_='author_name') or article_info.find('span', class_='byline basic')\n",
    "\n",
    "    if (author != None):\n",
    "        if (author.find('a') != None):\n",
    "            author = author.find('a')\n",
    "            author_name = author.get_text().lower()\n",
    "        else :\n",
    "            author_name = author.get_text().lower()\n",
    "    else :\n",
    "        author_name = 'not found'\n",
    "    \n",
    "    \n",
    "\n",
    "    # 6. to find the article topic\n",
    "    footer = soup.find('footer', class_='article-topics')\n",
    "    topic = footer.get_text().split(': ')[1]\n",
    "    \n",
    "    # 7. to find the channel\n",
    "    channel = soup.find('article')['data-channel'].strip().lower()\n",
    "    \n",
    "    '''\n",
    "    print('title = ', title, type(title))\n",
    "    print('time = ', year, \"/\", month, \"/\",day, \" \",hour, \":\",minute, \":\",second, type(year))\n",
    "    print('number of images = ', num_img, type(num_img))\n",
    "    print('number of videos = ', num_video, type(num_video))\n",
    "    print('author_name = ', author_name, type(author_name))\n",
    "    print('topic = ', topic, type(topic))\n",
    "    print('channel = ', channel, type(channel))\n",
    "    '''\n",
    "    \n",
    "    # 8. to find the content length\n",
    "    content = soup.body.find('section', class_='article-content').get_text()\n",
    "    len_content = len(content)\n",
    "\n",
    "    # print('topic = ', topic, type(topic))\n",
    "\n",
    "    return title, author_name, channel, topic, year, month, date, day, is_weekend, hour, minute, second, num_img, num_video, len_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "\n",
    "for content in (train_data['Page content']):\n",
    "    feature_list.append(preprocessor(content))\n",
    "\n",
    "for content in (test_data['Page content']):\n",
    "    feature_list.append(preprocessor(content))\n",
    "\n",
    "df_all = pd.DataFrame(\n",
    "        feature_list, \n",
    "        columns=['title', 'author_name', 'channel', 'topic', 'year', 'month', 'date', 'day', 'is_weekend', 'hour', 'minute', 'second', 'num_img', 'num_video', 'len_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author_name</th>\n",
       "      <th>channel</th>\n",
       "      <th>topic</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>num_img</th>\n",
       "      <th>num_video</th>\n",
       "      <th>len_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasa's grand challenge: stop asteroids from de...</td>\n",
       "      <td>clara moskowitz</td>\n",
       "      <td>world</td>\n",
       "      <td>Asteroid, Asteroids, challenge, Earth, Space, ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google's new open source patent pledge: we won...</td>\n",
       "      <td>christina warren</td>\n",
       "      <td>tech</td>\n",
       "      <td>Apps and Software, Google, open source, opn pl...</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ballin': 2014 nfl draft picks get to choose th...</td>\n",
       "      <td>sam laird</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Entertainment, NFL, NFL Draft, Sports, Televis...</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cameraperson fails deliver slapstick laughs</td>\n",
       "      <td>sam laird</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Sports, Video, Videos, Watercooler</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nfl star helps young fan prove friendship with...</td>\n",
       "      <td>connor finnegan</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Entertainment, instagram, instagram video, NFL...</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>8919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title       author_name  \\\n",
       "0  nasa's grand challenge: stop asteroids from de...   clara moskowitz   \n",
       "1  google's new open source patent pledge: we won...  christina warren   \n",
       "2  ballin': 2014 nfl draft picks get to choose th...         sam laird   \n",
       "3        cameraperson fails deliver slapstick laughs         sam laird   \n",
       "4  nfl star helps young fan prove friendship with...   connor finnegan   \n",
       "\n",
       "         channel                                              topic  year  \\\n",
       "0          world  Asteroid, Asteroids, challenge, Earth, Space, ...  2013   \n",
       "1           tech  Apps and Software, Google, open source, opn pl...  2013   \n",
       "2  entertainment  Entertainment, NFL, NFL Draft, Sports, Televis...  2014   \n",
       "3    watercooler                Sports, Video, Videos, Watercooler   2013   \n",
       "4  entertainment  Entertainment, instagram, instagram video, NFL...  2014   \n",
       "\n",
       "   month  date  day  is_weekend  hour  minute  second  num_img  num_video  \\\n",
       "0      6    19    3           0    15       4      30        1          0   \n",
       "1      3    28    4           0    17      40      55        2          0   \n",
       "2      5     7    3           0    19      15      20        2         25   \n",
       "3     10    11    5           0     2      26      50        1         21   \n",
       "4      4    17    4           0     3      31      43       52          1   \n",
       "\n",
       "   len_content  \n",
       "0         3591  \n",
       "1         1843  \n",
       "2         6646  \n",
       "3         1821  \n",
       "4         8919  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 . Preprocessing - tokenization\n",
    "\n",
    "To split the text corpora into individual elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 . Preprocessing - Word Stemming\n",
    "\n",
    "There are two ways of word stemming\n",
    "\n",
    "1. PorterStemmer(Stemming): break the word in rule-besed way, which will lead to the probelm of overstemming\n",
    "\n",
    "2. WordNetLemmatizer(Lemmatization): Stem the words will better performance, while time-consuming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university,', 'universal,', 'university']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def word_stemming(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    lm = WordNetLemmatizer()\n",
    "    words = re.split('\\s', text.strip())\n",
    "    lemmatized_words = [lm.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "print(word_stemming('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university,', 'universal,', 'univers']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "print(tokenizer_stem('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 Preprocessing - Stop-Word Removal\n",
    "\n",
    "儘管刪除停用詞在某些情況下（例如 BoW 和特徵哈希）可以有益於簡化表示，並可能提高文字分析的準確性，但並不總是必要，特別是在使用 TF-IDF 時。是否刪除停用詞應基於文本分析任務的具體要求以及資料集的特性來進行決策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/student/mr111//jhliu22/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def stop_word_removal(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(stop_word_removal('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-4 Create TF-IDF feature representation ([ref](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "text_transformer = ColumnTransformer(\n",
    "    [\n",
    "        #('title preprocess', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [0]),            # to process the title paragraph\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,1), lowercase=False), [0]),\n",
    "        #('channel process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [1]),\n",
    "        ('topic name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,1), lowercase=False), [1]),\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "catboost_transformer =  ColumnTransformer(\n",
    "    [\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,1), lowercase=False), [0]),            # to process the title paragraph\n",
    "        ('topic name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,1), lowercase=False), [1])\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "to: 2.52\n",
      "the: 2.58\n",
      "in: 2.96\n",
      "a: 3.03\n",
      "of: 3.07\n",
      "for: 3.10\n",
      "and: 3.44\n",
      "is: 3.51\n",
      "on: 3.54\n",
      "your: 3.60\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "the: 1142.4830180213792\n",
      "to: 1109.6698985304176\n",
      "a: 795.7866740412087\n",
      "in: 787.5419357082401\n",
      "of: 746.7665375362841\n",
      "for: 734.8807138633431\n",
      "and: 555.5640584744767\n",
      "your: 551.5848064261177\n",
      "is: 544.621163447621\n",
      "you: 533.4917456688497\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False)\n",
    "tfidf.fit(df_all['title'])\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names_out()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(df_all['title']).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones((1, tfidf_sum.shape[0])))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model training\n",
    "\n",
    "- XGBoost\n",
    "\n",
    "- LightGBM\n",
    "\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>topic</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour</th>\n",
       "      <th>num_video</th>\n",
       "      <th>len_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clara moskowitz</td>\n",
       "      <td>Asteroid, Asteroids, challenge, Earth, Space, ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>christina warren</td>\n",
       "      <td>Apps and Software, Google, open source, opn pl...</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>Entertainment, NFL, NFL Draft, Sports, Televis...</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>Sports, Video, Videos, Watercooler</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connor finnegan</td>\n",
       "      <td>Entertainment, instagram, instagram video, NFL...</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_name                                              topic  year  \\\n",
       "0   clara moskowitz  Asteroid, Asteroids, challenge, Earth, Space, ...  2013   \n",
       "1  christina warren  Apps and Software, Google, open source, opn pl...  2013   \n",
       "2         sam laird  Entertainment, NFL, NFL Draft, Sports, Televis...  2014   \n",
       "3         sam laird                Sports, Video, Videos, Watercooler   2013   \n",
       "4   connor finnegan  Entertainment, instagram, instagram video, NFL...  2014   \n",
       "\n",
       "   month  date  day  is_weekend  hour  num_video  len_content  \n",
       "0      6    19    3           0    15          0         3591  \n",
       "1      3    28    4           0    17          0         1843  \n",
       "2      5     7    3           0    19         25         6646  \n",
       "3     10    11    5           0     2         21         1821  \n",
       "4      4    17    4           0     3          1         8919  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_all.loc[:, [\n",
    "                    #'title', \n",
    "                    'author_name', \n",
    "                    #'channel', \n",
    "                    'topic', \n",
    "                    'year', \n",
    "                    'month',\n",
    "                    'date', \n",
    "                    'day', \n",
    "                    'is_weekend',\n",
    "                    'hour', \n",
    "                    # 'minute', \n",
    "                    # 'second', \n",
    "                    # 'num_img', \n",
    "                    'num_video', \n",
    "                    'len_content'\n",
    "                    ]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "[['clara moskowitz'\n",
      "  'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ' 2013 ...\n",
      "  15 0 3591]\n",
      " ['christina warren'\n",
      "  'Apps and Software, Google, open source, opn pledge, patent lawsuit theater, software patents, Tech, U.S. '\n",
      "  2013 ... 17 0 1843]\n",
      " ['sam laird' 'Entertainment, NFL, NFL Draft, Sports, Television ' 2014\n",
      "  ... 19 25 6646]\n",
      " ...\n",
      " ['christine erickson' 'Food, hot dogs, humor, Photography, Watercooler '\n",
      "  2014 ... 12 0 1274]\n",
      " ['seth fiegerman' 'Business, marissa mayer, Media, stocks, Yahoo ' 2013\n",
      "  ... 20 0 2657]\n",
      " ['megan ranney' 'austin, Business, CurioCity, Small Business, Startups '\n",
      "  2014 ... 18 0 3027]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = df.values[:train_data.shape[0]]\n",
    "y_train = train_data['Popularity'].values\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = df.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(type(X_train))\n",
    "print(X_train)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To construct the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def training(clf):\n",
    "    clf_cv = cross_validate(clf, X_train, y_train, scoring='roc_auc', return_train_score=True, return_estimator=True)\n",
    "    print('train score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(clf_cv['train_score']), np.std(clf_cv['train_score'])))\n",
    "    print('valid score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(clf_cv['test_score']), np.std(clf_cv['test_score'])))\n",
    "\n",
    "    clf.fit(X_train_split, y_train_split)\n",
    "    train_score = roc_auc_score(\n",
    "        y_train_split, clf.predict_proba(X_train_split)[:, 1])\n",
    "    valid_score = (roc_auc_score(\n",
    "        y_valid_split, clf.predict_proba(X_valid_split)[:, 1]))\n",
    "    \n",
    "    return clf, np.mean(clf_cv['train_score']),  np.mean(clf_cv['test_score'])\n",
    "    # return clf, train_score, valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1-1. Grid Sizing for XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# param_grid_xgb = {\n",
    "#     'gamma' : [0, 0.5, 1, 1.5, 2],\n",
    "#     'lambda' : [1.5, 2, 2.5, 3],\n",
    "#     'n_estimators': [100, 120, 140, 160, 180],\n",
    "#     'max_depth': [6, 8, 10, 12, 14],\n",
    "#     'learning_rate' : [0.14, 0.15, 0.16]  \n",
    "# }\n",
    "\n",
    "# best_xgb_param, best_xgb = grid_search_cv(text_transformer, XGBClassifier(n_jobs=-1), param_grid_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1-2. Training for XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# param_grid_lgbm = {\n",
    "#     'gamma' : 1,\n",
    "#     'lambda' : 2.5,\n",
    "#     'n_estimators': 100,\n",
    "#     'max_depth': 8,\n",
    "#     'learning_rate' : 0.14,\n",
    "#     'n_jobs' : -1\n",
    "# }\n",
    "\n",
    "# # '**' 是一種解包（unpacking）操作符，它可以用於將字典中的鍵值對以關鍵字參數的方式傳遞給函數或方法\n",
    "# xgboost = Pipeline([('vect', text_transformer),\n",
    "#                     ('clf', XGBClassifier(**param_grid_lgbm))])\n",
    "\n",
    "# training(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3 CatBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-1 Use simple for-loop to tuning the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# catboost = Pipeline([('ct', catboost_transformer),\n",
    "#                      ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=290, learning_rate=0.06))])\n",
    "# catboost, train_error, valid_error = training(catboost)\n",
    "\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "with open('../catboost_log/tree_num_tree_depth_vs_lr.log','w') as file:\n",
    "    for tree_num in range(100,0,100):\n",
    "        for tree_depth in range(3, 11): # depth from 3 to 10 (8)\n",
    "            for i in range(1,11): # lr from 0.01 to 0.10 (10)\n",
    "                print (\"tree_num: \", tree_num, \"tree_depth :\", tree_depth, \" learning rate: \", i*0.01)\n",
    "                catboost = Pipeline([('ct', catboost_transformer),\n",
    "                                    ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=tree_num, depth = tree_depth, learning_rate=i*0.01, random_state = 0))])\n",
    "                catboost, train_acc, valid_acc = training(catboost)\n",
    "                train_acc_list.append(train_acc)\n",
    "                valid_acc_list.append(valid_acc)\n",
    "                write_str = \"tree_num: \" + str(tree_num) + \" tree_depth:\"+ str(tree_depth) + \" learning_rate: \" +  str(i*0.01) + \" train_acc: \" + str(train_acc) + \" valid_acc: \" + str(valid_acc) + '\\n'\n",
    "                file.write (write_str)\n",
    "                file.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-2 Use matplot to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# train_acc_list = []\n",
    "# valid_acc_list = []\n",
    "\n",
    "# with open ('../catboost_log/tree_num_tree_depth_vs_lr.log','r') as file:\n",
    "#     for line in file:\n",
    "#         if 'train_acc' in line and 'valid_acc' in line:\n",
    "#             train_acc = float(line.split('train_acc: ')[1].split(' valid_acc: ')[0])\n",
    "#             valid_acc = float(line.split('valid_acc: ')[1])\n",
    "#             train_acc_list.append(train_acc)\n",
    "#             valid_acc_list.append(valid_acc)\n",
    "\n",
    "# print(\"Train Accuracy List: \", train_acc_list[:5])\n",
    "# print(\"Valid Accuracy List: \", valid_acc_list[:5])\n",
    "\n",
    "# tree_num_list = list(range(100,1100,100))\n",
    "# tree_depth_list = list(range(3,11))\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# best_result_list = []\n",
    "# for i in range(5):\n",
    "#     tree_num_train_list = train_acc_list[i*80:(i+1)*80] # there are 80 data in each list\n",
    "#     tree_num_valid_list = valid_acc_list[i*80:(i+1)*80]\n",
    "#     max_valid_acc = max(tree_num_valid_list)\n",
    "#     max_index = tree_num_valid_list.index(max_valid_acc) # the index should be 0 ~ 79\n",
    "#     train_acc = tree_num_train_list[max_index]\n",
    "#     max_lr = (max_index % 10) * 0.01\n",
    "#     max_tree_depth  = (max_index-1) // 10 + 3\n",
    "#     best_result_list.append((max_tree_depth, max_lr, max_valid_acc))\n",
    "#     print(\"In tree num: \", str((i+1)*100), \", the best param (tree_depth, max_lr) is: (\", max_tree_depth,\", \" ,max_lr, \"), and its train/valid acc = \",train_acc,'/', max_valid_acc)\n",
    "\n",
    "\n",
    "#     # plt.subplot(5,2,i+1)\n",
    "#     # # 繪製圖形\n",
    "#     # plt.plot(tree_depth_list, train_iter_300_list, marker='o', linestyle='--', color='b', label='Train acc')\n",
    "#     # plt.plot(tree_depth_list, valid_iter_300_list, marker='o', linestyle='--', color='r', label='Valid acc')\n",
    "\n",
    "#     # # 添加標籤和標題\n",
    "#     # plt.xlabel('tree_depth')\n",
    "#     # plt.ylabel('Accuracy')\n",
    "#     # title_name = 'Training and Validation Accuracy vs tree_depth on tree number = ' + str((i+1)*100)\n",
    "#     # plt.title(title_name)\n",
    "#     # plt.legend()\n",
    "\n",
    "# # 顯示圖形\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-3 Use grid search to select best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_cv(ct, clf, param_grid):\n",
    "    X_train_ct = ct.fit_transform(X_train)\n",
    "    gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=16, cv=5, return_train_score=True, verbose=2)\n",
    "    gs.fit(X_train_ct, y_train)\n",
    "    results, idx = gs.cv_results_, gs.best_index_\n",
    "    print('train score: {:.5f} (+/-{:.5f})'.format(results['mean_train_score'][idx], results['std_train_score'][idx]))\n",
    "    print('valid score: {:.5f} (+/-{:.5f})'.format(results['mean_test_score'][idx], results['std_test_score'][idx]))\n",
    "    print('best params:', gs.best_params_)\n",
    "    return gs.best_params_, gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=400; total time= 6.6min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=400; total time= 6.7min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=400; total time= 6.7min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=400; total time= 6.7min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=400; total time= 6.8min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=400; total time= 6.8min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=500; total time= 8.3min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=500; total time= 8.4min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=500; total time= 8.4min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=500; total time= 8.4min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=500; total time= 8.5min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=600; total time=10.0min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=600; total time=10.1min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=600; total time=10.2min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=600; total time=10.2min\n",
      "[CV] END ....depth=10, learning_rate=0.001, n_estimators=600; total time=10.2min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=400; total time= 6.9min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=400; total time= 6.8min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=400; total time= 6.9min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=400; total time= 6.9min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=500; total time= 8.6min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=500; total time= 8.7min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=500; total time= 8.4min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=400; total time= 6.7min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=500; total time= 8.5min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=500; total time= 8.5min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=400; total time= 6.8min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=600; total time=10.0min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=600; total time=10.3min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=600; total time=10.0min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=600; total time=10.0min\n",
      "[CV] END ....depth=10, learning_rate=0.005, n_estimators=600; total time=10.1min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=400; total time= 6.8min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=400; total time= 6.7min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=400; total time= 6.8min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=500; total time= 8.3min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=500; total time= 8.4min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=500; total time= 8.4min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=500; total time= 8.3min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=500; total time= 8.5min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=400; total time= 6.8min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=400; total time= 6.8min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=400; total time= 6.9min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=600; total time=10.2min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=600; total time=10.2min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=600; total time=10.2min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=400; total time= 6.8min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=400; total time= 6.8min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=600; total time=10.3min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=500; total time= 8.6min\n",
      "[CV] END .....depth=10, learning_rate=0.01, n_estimators=600; total time=10.2min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=500; total time= 8.8min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=500; total time= 8.9min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=500; total time= 8.9min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=500; total time= 8.9min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=600; total time=10.6min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=400; total time= 9.7min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=400; total time= 9.7min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=400; total time= 9.9min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=400; total time=10.0min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=600; total time=10.9min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=600; total time=10.8min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=600; total time=11.0min\n",
      "[CV] END .....depth=10, learning_rate=0.05, n_estimators=600; total time=10.9min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=400; total time=10.1min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=500; total time=12.5min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=500; total time=12.7min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=500; total time=13.2min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=500; total time=13.2min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=500; total time=13.2min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=400; total time=11.0min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=400; total time=10.8min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=400; total time=10.9min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=400; total time=10.9min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=400; total time=11.1min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=600; total time=15.8min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=600; total time=16.1min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=600; total time=15.9min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=500; total time=13.6min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=600; total time=15.9min\n",
      "[CV] END ....depth=11, learning_rate=0.001, n_estimators=600; total time=16.2min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=500; total time=13.7min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=500; total time=13.6min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=500; total time=13.5min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=500; total time=13.7min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=400; total time=11.1min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=400; total time=11.3min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=600; total time=16.5min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=400; total time=11.3min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=400; total time=11.1min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=400; total time=11.3min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=600; total time=16.2min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=600; total time=16.7min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=600; total time=16.6min\n",
      "[CV] END ....depth=11, learning_rate=0.005, n_estimators=600; total time=16.7min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=500; total time=13.8min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=500; total time=13.9min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=500; total time=13.7min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=500; total time=14.0min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=500; total time=13.7min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=400; total time=11.1min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=400; total time=11.1min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=400; total time=11.2min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=600; total time=16.3min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=400; total time=10.9min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=400; total time=11.1min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=600; total time=16.5min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=600; total time=16.4min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=600; total time=16.0min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=500; total time=13.5min\n",
      "[CV] END .....depth=11, learning_rate=0.01, n_estimators=600; total time=16.0min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=500; total time=13.3min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=500; total time=13.3min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=500; total time=12.9min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=500; total time=12.8min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=600; total time=13.7min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=600; total time=12.4min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=600; total time=12.0min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=600; total time=12.1min\n",
      "[CV] END .....depth=11, learning_rate=0.05, n_estimators=600; total time=11.9min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=400; total time=30.3min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=400; total time=30.7min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=400; total time=30.9min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=400; total time=31.4min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=400; total time=30.6min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=500; total time=38.1min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=400; total time=29.8min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=500; total time=38.1min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=500; total time=38.0min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=500; total time=39.7min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=500; total time=38.4min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=600; total time=44.3min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=600; total time=44.8min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=600; total time=45.4min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=600; total time=45.9min\n",
      "[CV] END ....depth=12, learning_rate=0.001, n_estimators=600; total time=46.9min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=400; total time=29.6min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=400; total time=29.7min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=400; total time=30.7min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=400; total time=31.0min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=500; total time=37.2min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=500; total time=36.9min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=500; total time=37.0min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=500; total time=37.6min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=500; total time=37.2min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=400; total time=29.4min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=600; total time=44.5min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=400; total time=30.2min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=600; total time=44.7min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=400; total time=29.1min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=400; total time=29.7min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=400; total time=30.1min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=600; total time=43.4min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=600; total time=44.6min\n",
      "[CV] END ....depth=12, learning_rate=0.005, n_estimators=600; total time=44.2min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=500; total time=38.6min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=500; total time=36.6min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=500; total time=37.4min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=500; total time=37.9min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=500; total time=38.2min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=400; total time=29.5min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=400; total time=31.0min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=400; total time=31.5min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=600; total time=44.3min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=400; total time=30.9min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=400; total time=28.5min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=600; total time=42.9min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=600; total time=43.0min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=600; total time=43.0min\n",
      "[CV] END .....depth=12, learning_rate=0.01, n_estimators=600; total time=43.5min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=500; total time=33.1min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=500; total time=33.0min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=500; total time=29.9min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=500; total time=24.0min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=500; total time=23.6min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=600; total time=24.7min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=600; total time=22.2min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=600; total time=21.0min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=600; total time=18.2min\n",
      "[CV] END .....depth=12, learning_rate=0.05, n_estimators=600; total time=18.9min\n",
      "train score: 0.74827 (+/-0.00440)\n",
      "valid score: 0.60161 (+/-0.00908)\n",
      "best params: {'depth': 12, 'learning_rate': 0.01, 'n_estimators': 600}\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_grid_catboost = {\n",
    "    'learning_rate' : [0.001, 0.005, 0.01, 0.05], \n",
    "    'n_estimators' : [400,500,600],\n",
    "    'depth': [10,11,12],\n",
    "}\n",
    "\n",
    "best_cat_params, best_estimator_ = grid_search_cv(text_transformer, CatBoostClassifier(verbose=False, eval_metric='AUC',random_state=0), param_grid_catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use specified param to get the y_pred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.68975 (+/-0.00182)\n",
      "valid score: 0.59867 (+/-0.00856)\n"
     ]
    }
   ],
   "source": [
    "catboost = Pipeline([('ct', catboost_transformer),\n",
    "                                    ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=500, depth = 10, learning_rate=0.01, random_state = 0))])\n",
    "catboost, train_acc, valid_acc = training(catboost)\n",
    "\n",
    "y_score = catboost.predict_proba(X_test)[:, 1]\n",
    "df_pred = pd.DataFrame({'Id': test_data['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('../output/test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
