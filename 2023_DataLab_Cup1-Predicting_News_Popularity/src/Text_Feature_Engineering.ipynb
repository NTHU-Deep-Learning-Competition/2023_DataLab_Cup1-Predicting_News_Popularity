{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>DataLab Cup 1: Text Feature Engineering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. To load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../dataset/train.csv')\n",
    "test_data  = pd.read_csv('../dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. To extract the features from the dataset\n",
    "\n",
    "將一些我們想要用到的feature從dataset中提取出來。以下為提取的特徵:\n",
    "\n",
    "- title\n",
    "- time(year/month/day/hour/minute/second)\n",
    "- number of images (num_img)\n",
    "- number of videos (num_video)\n",
    "- author name\n",
    "- topic\n",
    "- channel\n",
    "- length of content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "* beautiful soup\n",
    "    - conda install -c conda-forge beautifulsoup4\n",
    "    \n",
    "<br>\n",
    "\n",
    "* vadersentiment\n",
    "    - conda install -c conda-forge vadersentiment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# to get the attribute of the 'title', 'year/month/date/day/hour/minute/second/is_weekend', 'num_img', 'num_video', 'author name', 'topic', 'channel', 'content length', 'title_sentiment'\n",
    "\n",
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # 1. to find the 'title' (body > h1)\n",
    "    title = soup.find('h1').string.strip().lower()\n",
    "\n",
    "    # 2. to find time(body > div > span > time)\n",
    "\n",
    "    \n",
    "    date_string = soup.find('time')\n",
    "    try:\n",
    "        date_string = date_string['datetime']\n",
    "    except:\n",
    "        date_string = 'wed, 10 oct 2014 15:00:43 +0000'\n",
    "        \n",
    "    date_string = date_string.strip().lower()\n",
    "    datetimes = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S %z')\n",
    "    \n",
    "    \n",
    "    year = datetimes.year\n",
    "    month = datetimes.month\n",
    "    date = datetimes.day\n",
    "    day = pd.Timestamp(str(year)+'-'+str(month)+'-'+str(date)).dayofweek+1\n",
    "    is_weekend = 1 if (day==6 or day==7) else 0\n",
    "    hour = datetimes.hour\n",
    "    minute = datetimes.minute\n",
    "    second = datetimes.second\n",
    "    \n",
    "\n",
    "    # 3. to find the number of images\n",
    "    num_img  = len(soup.find_all('img'))\n",
    "    \n",
    "\n",
    "    # 4. to find the number of videos\n",
    "    num_video = len(soup.find_all('iframe'))\n",
    "    \n",
    "\n",
    "    # 5. to find the author name\n",
    "    article_info = soup.find('div', class_='article-info')\n",
    "    author = article_info.find('span', class_='author_name') or article_info.find('span', class_='byline basic')\n",
    "\n",
    "    if (author != None):\n",
    "        if (author.find('a') != None):\n",
    "            author = author.find('a')\n",
    "            author_name = author.get_text().lower()\n",
    "        else :\n",
    "            author_name = author.get_text().lower()\n",
    "    else :\n",
    "        author_name = 'not found'\n",
    "    \n",
    "    \n",
    "\n",
    "    # 6. to find the article topic\n",
    "    footer = soup.find('footer', class_='article-topics')\n",
    "    topic = footer.get_text().split(': ')[1]\n",
    "    \n",
    "    # 7. to find the channel\n",
    "    channel = soup.find('article')['data-channel'].strip().lower()\n",
    "    \n",
    "    '''\n",
    "    print('title = ', title, type(title))\n",
    "    print('time = ', year, \"/\", month, \"/\",day, \" \",hour, \":\",minute, \":\",second, type(year))\n",
    "    print('number of images = ', num_img, type(num_img))\n",
    "    print('number of videos = ', num_video, type(num_video))\n",
    "    print('author_name = ', author_name, type(author_name))\n",
    "    print('topic = ', topic, type(topic))\n",
    "    print('channel = ', channel, type(channel))\n",
    "    '''\n",
    "    \n",
    "    # 8. to find the content length\n",
    "    content = soup.body.find('section', class_='article-content').get_text()\n",
    "    len_content = len(content)\n",
    "\n",
    "    # print('topic = ', topic, type(topic))\n",
    "    \n",
    "    # 9. to find the sentiment of title\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    title_sentiment = analyzer.polarity_scores(topic)\n",
    "    sentiment_neg = title_sentiment['neg']\n",
    "    sentiment_neu = title_sentiment['neu']\n",
    "    sentiment_pos = title_sentiment['pos']\n",
    "    sentiment_compound = title_sentiment['compound']\n",
    "\n",
    "    return title, author_name, channel, topic, year, month, date, day, is_weekend, hour, minute, second, num_img, num_video, len_content, sentiment_neg, sentiment_neu, sentiment_pos, sentiment_compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_list = []\n",
    "feature_test_list = []\n",
    "\n",
    "for content in (train_data['Page content']):\n",
    "    feature_train_list.append(preprocessor(content))\n",
    "for content in (test_data['Page content']):\n",
    "    feature_train_list.append(preprocessor(content))\n",
    "\n",
    "df_all = pd.DataFrame(\n",
    "        feature_train_list, \n",
    "        columns=['title', 'author_name', 'channel', 'topic', 'year', 'month', 'date', 'day', 'is_weekend', 'hour', 'minute', 'second', 'num_img', 'num_video', 'len_content', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'sentiment_compound'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 . Preprocessing - Word Stemming\n",
    "\n",
    "There are two ways of word stemming\n",
    "\n",
    "1. PorterStemmer(Stemming): break the word in rule-besed way, which will lead to the probelm of overstemming\n",
    "\n",
    "2. WordNetLemmatizer(Lemmatization): Stem the words will better performance, while time-consuming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university,', 'universal,', 'university']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/student/mr111//mfhsieh22/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /users/student/mr111//mfhsieh22/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.data.path.append('/home/mfhsieh/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def word_stemming(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    lm = WordNetLemmatizer()\n",
    "    words = re.split('\\s', text.strip())\n",
    "    lemmatized_words = [lm.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "print(word_stemming('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 Preprocessing - Stop-Word Removal\n",
    "\n",
    "儘管刪除停用詞在某些情況下（例如 BoW 和特徵哈希）可以有益於簡化表示，並可能提高文字分析的準確性，但並不總是必要，特別是在使用 TF-IDF 時。是否刪除停用詞應基於文本分析任務的具體要求以及資料集的特性來進行決策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-5 Create TF-IDF feature representation ([ref](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ngram_range_ = [(1,2), (1,1)]\n",
    "\n",
    "text_transformer = ColumnTransformer(\n",
    "    [\n",
    "        #('title preprocess', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [0]),\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [0]),\n",
    "        #('channel process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [1]),\n",
    "        ('topic name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,1), lowercase=False), [1]),\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model training\n",
    "\n",
    "- XGBoost\n",
    "\n",
    "- LightGBM\n",
    "\n",
    "- CatBoost\n",
    "\n",
    "- AdaBoost\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 14)\n",
      "<class 'numpy.ndarray'>\n",
      "[['clara moskowitz'\n",
      "  'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ' 2013 ...\n",
      "  0.822 0.178 0.0772]\n",
      " ['christina warren'\n",
      "  'Apps and Software, Google, open source, opn pledge, patent lawsuit theater, software patents, Tech, U.S. '\n",
      "  2013 ... 0.881 0.0 -0.2263]\n",
      " ['sam laird' 'Entertainment, NFL, NFL Draft, Sports, Television ' 2014\n",
      "  ... 0.641 0.359 0.4215]\n",
      " ...\n",
      " ['christine erickson' 'Food, hot dogs, humor, Photography, Watercooler '\n",
      "  2014 ... 0.704 0.296 0.2732]\n",
      " ['seth fiegerman' 'Business, marissa mayer, Media, stocks, Yahoo ' 2013\n",
      "  ... 1.0 0.0 0.0]\n",
      " ['megan ranney' 'austin, Business, CurioCity, Small Business, Startups '\n",
      "  2014 ... 1.0 0.0 0.0]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "remaining_dict_xgboost = [  #'title', \n",
    "                            'author_name', \n",
    "                            #'channel', \n",
    "                            'topic', \n",
    "                            'year', \n",
    "                            'month',\n",
    "                            'date', \n",
    "                            'day', \n",
    "                            'is_weekend',\n",
    "                            'hour', \n",
    "                            # 'minute', \n",
    "                            # 'second', \n",
    "                            # 'num_img', \n",
    "                            'num_video', \n",
    "                            'len_content',\n",
    "                            'sentiment_neg', \n",
    "                            'sentiment_neu', \n",
    "                            'sentiment_pos', \n",
    "                            'sentiment_compound'\n",
    "                            ]\n",
    "\n",
    "df_xgboost = df_all.loc[:, remaining_dict_xgboost]\n",
    "\n",
    "X_xgboost_train = df_xgboost.values[:train_data.shape[0]]\n",
    "y_xgboost_train = train_data['Popularity'].values\n",
    "y_xgboost_train[y_xgboost_train==-1] = 0\n",
    "X_xgboost_test = df_xgboost.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_xgboost_train.shape)\n",
    "print(type(X_xgboost_train))\n",
    "print(X_xgboost_train)\n",
    "print(y_xgboost_train.shape)\n",
    "print(type(y_xgboost_train))\n",
    "print(y_xgboost_train)\n",
    "\n",
    "X_xgboost_train_split, X_xgboost_valid_split, y_xgboost_train_split, y_xgboost_valid_split = train_test_split(X_xgboost_train, y_xgboost_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LightGbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "[['clara moskowitz'\n",
      "  'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ' 2013 ...\n",
      "  15 0 3591]\n",
      " ['christina warren'\n",
      "  'Apps and Software, Google, open source, opn pledge, patent lawsuit theater, software patents, Tech, U.S. '\n",
      "  2013 ... 17 0 1843]\n",
      " ['sam laird' 'Entertainment, NFL, NFL Draft, Sports, Television ' 2014\n",
      "  ... 19 25 6646]\n",
      " ...\n",
      " ['christine erickson' 'Food, hot dogs, humor, Photography, Watercooler '\n",
      "  2014 ... 12 0 1274]\n",
      " ['seth fiegerman' 'Business, marissa mayer, Media, stocks, Yahoo ' 2013\n",
      "  ... 20 0 2657]\n",
      " ['megan ranney' 'austin, Business, CurioCity, Small Business, Startups '\n",
      "  2014 ... 18 0 3027]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "remaining_dict_lightgbm = [  #'title', \n",
    "                            'author_name', \n",
    "                            #'channel', \n",
    "                            'topic', \n",
    "                            'year', \n",
    "                            'month',\n",
    "                            'date', \n",
    "                            'day', \n",
    "                            'is_weekend',\n",
    "                            'hour', \n",
    "                            # 'minute', \n",
    "                            # 'second', \n",
    "                            # 'num_img', \n",
    "                            'num_video', \n",
    "                            'len_content',\n",
    "                            # 'sentiment_neg', \n",
    "                            # 'sentiment_neu', \n",
    "                            # 'sentiment_pos', \n",
    "                            # 'sentiment_compound'\n",
    "                            ]\n",
    "\n",
    "df_lightgbm = df_all.loc[:, remaining_dict_lightgbm]\n",
    "\n",
    "X_lightgbm_train = df_lightgbm.values[:train_data.shape[0]]\n",
    "y_lightgbm_train = train_data['Popularity'].values\n",
    "y_lightgbm_train[y_lightgbm_train==-1] = 0\n",
    "X_lightgbm_test = df_lightgbm.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_lightgbm_train.shape)\n",
    "print(type(X_lightgbm_train))\n",
    "print(X_lightgbm_train)\n",
    "print(y_lightgbm_train.shape)\n",
    "print(type(y_lightgbm_train))\n",
    "print(y_lightgbm_train)\n",
    "\n",
    "X_lightgbm_train_split, X_lightgbm_valid_split, y_lightgbm_train_split, y_lightgbm_valid_split = train_test_split(X_lightgbm_train, y_lightgbm_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "[['clara moskowitz'\n",
      "  'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ' 2013 ...\n",
      "  15 0 3591]\n",
      " ['christina warren'\n",
      "  'Apps and Software, Google, open source, opn pledge, patent lawsuit theater, software patents, Tech, U.S. '\n",
      "  2013 ... 17 0 1843]\n",
      " ['sam laird' 'Entertainment, NFL, NFL Draft, Sports, Television ' 2014\n",
      "  ... 19 25 6646]\n",
      " ...\n",
      " ['christine erickson' 'Food, hot dogs, humor, Photography, Watercooler '\n",
      "  2014 ... 12 0 1274]\n",
      " ['seth fiegerman' 'Business, marissa mayer, Media, stocks, Yahoo ' 2013\n",
      "  ... 20 0 2657]\n",
      " ['megan ranney' 'austin, Business, CurioCity, Small Business, Startups '\n",
      "  2014 ... 18 0 3027]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "remaining_dict_catboost = [  #'title', \n",
    "                            'author_name', \n",
    "                            #'channel', \n",
    "                            'topic', \n",
    "                            'year', \n",
    "                            'month',\n",
    "                            'date', \n",
    "                            'day', \n",
    "                            'is_weekend',\n",
    "                            'hour', \n",
    "                            # 'minute', \n",
    "                            # 'second', \n",
    "                            # 'num_img', \n",
    "                            'num_video', \n",
    "                            'len_content',\n",
    "                            # 'sentiment_neg', \n",
    "                            # 'sentiment_neu', \n",
    "                            # 'sentiment_pos', \n",
    "                            # 'sentiment_compound'\n",
    "                            ]\n",
    "\n",
    "df_catboost = df_all.loc[:, remaining_dict_catboost]\n",
    "\n",
    "X_catboost_train = df_catboost.values[:train_data.shape[0]]\n",
    "y_catboost_train = train_data['Popularity'].values\n",
    "y_catboost_train[y_catboost_train==-1] = 0\n",
    "X_catboost_test = df_catboost.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_catboost_train.shape)\n",
    "print(type(X_catboost_train))\n",
    "print(X_catboost_train)\n",
    "print(y_catboost_train.shape)\n",
    "print(type(y_catboost_train))\n",
    "print(y_catboost_train)\n",
    "\n",
    "X_catboost_train_split, X_catboost_valid_split, y_catboost_train_split, y_catboost_valid_split = train_test_split(X_catboost_train, y_catboost_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "[['clara moskowitz'\n",
      "  'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ' 2013 ...\n",
      "  15 0 3591]\n",
      " ['christina warren'\n",
      "  'Apps and Software, Google, open source, opn pledge, patent lawsuit theater, software patents, Tech, U.S. '\n",
      "  2013 ... 17 0 1843]\n",
      " ['sam laird' 'Entertainment, NFL, NFL Draft, Sports, Television ' 2014\n",
      "  ... 19 25 6646]\n",
      " ...\n",
      " ['christine erickson' 'Food, hot dogs, humor, Photography, Watercooler '\n",
      "  2014 ... 12 0 1274]\n",
      " ['seth fiegerman' 'Business, marissa mayer, Media, stocks, Yahoo ' 2013\n",
      "  ... 20 0 2657]\n",
      " ['megan ranney' 'austin, Business, CurioCity, Small Business, Startups '\n",
      "  2014 ... 18 0 3027]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "remaining_dict_adaboost = [  #'title', \n",
    "                            'author_name', \n",
    "                            #'channel', \n",
    "                            'topic', \n",
    "                            'year', \n",
    "                            'month',\n",
    "                            'date', \n",
    "                            'day', \n",
    "                            'is_weekend',\n",
    "                            'hour', \n",
    "                            # 'minute', \n",
    "                            # 'second', \n",
    "                            # 'num_img', \n",
    "                            'num_video', \n",
    "                            'len_content',\n",
    "                            # 'sentiment_neg', \n",
    "                            # 'sentiment_neu', \n",
    "                            # 'sentiment_pos', \n",
    "                            # 'sentiment_compound'\n",
    "                            ]\n",
    "\n",
    "df_adaboost = df_all.loc[:, remaining_dict_adaboost]\n",
    "\n",
    "X_adaboost_train = df_adaboost.values[:train_data.shape[0]]\n",
    "y_adaboost_train = train_data['Popularity'].values\n",
    "y_adaboost_train[y_adaboost_train==-1] = 0\n",
    "X_adaboost_test = df_adaboost.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_adaboost_train.shape)\n",
    "print(type(X_adaboost_train))\n",
    "print(X_adaboost_train)\n",
    "print(y_adaboost_train.shape)\n",
    "print(type(y_adaboost_train))\n",
    "print(y_adaboost_train)\n",
    "\n",
    "X_adaboost_train_split, X_adaboost_valid_split, y_adaboost_train_split, y_adaboost_valid_split = train_test_split(X_adaboost_train, y_adaboost_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "[['clara moskowitz'\n",
      "  'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ' 2013 ...\n",
      "  15 0 3591]\n",
      " ['christina warren'\n",
      "  'Apps and Software, Google, open source, opn pledge, patent lawsuit theater, software patents, Tech, U.S. '\n",
      "  2013 ... 17 0 1843]\n",
      " ['sam laird' 'Entertainment, NFL, NFL Draft, Sports, Television ' 2014\n",
      "  ... 19 25 6646]\n",
      " ...\n",
      " ['christine erickson' 'Food, hot dogs, humor, Photography, Watercooler '\n",
      "  2014 ... 12 0 1274]\n",
      " ['seth fiegerman' 'Business, marissa mayer, Media, stocks, Yahoo ' 2013\n",
      "  ... 20 0 2657]\n",
      " ['megan ranney' 'austin, Business, CurioCity, Small Business, Startups '\n",
      "  2014 ... 18 0 3027]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "remaining_dict_RF = [  #'title', \n",
    "                            'author_name', \n",
    "                            #'channel', \n",
    "                            'topic', \n",
    "                            'year', \n",
    "                            'month',\n",
    "                            'date', \n",
    "                            'day', \n",
    "                            'is_weekend',\n",
    "                            'hour', \n",
    "                            # 'minute', \n",
    "                            # 'second', \n",
    "                            # 'num_img', \n",
    "                            'num_video', \n",
    "                            'len_content',\n",
    "                            # 'sentiment_neg', \n",
    "                            # 'sentiment_neu', \n",
    "                            # 'sentiment_pos', \n",
    "                            # 'sentiment_compound'\n",
    "                            ]\n",
    "\n",
    "df_RF = df_all.loc[:, remaining_dict_RF]\n",
    "\n",
    "X_RF_train = df_RF.values[:train_data.shape[0]]\n",
    "y_RF_train = train_data['Popularity'].values\n",
    "y_RF_train[y_RF_train==-1] = 0\n",
    "X_RF_test = df_RF.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_RF_train.shape)\n",
    "print(type(X_RF_train))\n",
    "print(X_RF_train)\n",
    "print(y_RF_train.shape)\n",
    "print(type(y_RF_train))\n",
    "print(y_RF_train)\n",
    "\n",
    "X_RF_train_split, X_RF_valid_split, y_RF_train_split, y_RF_valid_split = train_test_split(X_RF_train, y_RF_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 19)\n",
      "<class 'numpy.ndarray'>\n",
      "[[\"nasa's grand challenge: stop asteroids from destroying earth\"\n",
      "  'clara moskowitz' 'world' ... 0.822 0.178 0.0772]\n",
      " [\"google's new open source patent pledge: we won't sue unless attacked first\"\n",
      "  'christina warren' 'tech' ... 0.881 0.0 -0.2263]\n",
      " [\"ballin': 2014 nfl draft picks get to choose their own walk-out music\"\n",
      "  'sam laird' 'entertainment' ... 0.641 0.359 0.4215]\n",
      " ...\n",
      " ['14 dogs that frankly cannot take the heat' 'christine erickson'\n",
      "  'watercooler' ... 0.704 0.296 0.2732]\n",
      " ['yahoo earnings beat estimates, but core problems remain'\n",
      "  'seth fiegerman' 'business' ... 1.0 0.0 0.0]\n",
      " ['the winners of our #curiocity contest tour austin’s tech scene'\n",
      "  'megan ranney' 'small-business' ... 1.0 0.0 0.0]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "remaining_dict_voting = [   'title', \n",
    "                            'author_name', \n",
    "                            'channel', \n",
    "                            'topic', \n",
    "                            'year', \n",
    "                            'month',\n",
    "                            'date', \n",
    "                            'day', \n",
    "                            'is_weekend',\n",
    "                            'hour', \n",
    "                            'minute', \n",
    "                            'second', \n",
    "                            'num_img', \n",
    "                            'num_video', \n",
    "                            'len_content',\n",
    "                            'sentiment_neg', \n",
    "                            'sentiment_neu', \n",
    "                            'sentiment_pos', \n",
    "                            'sentiment_compound'\n",
    "                            ]\n",
    "\n",
    "df_voting = df_all.loc[:, remaining_dict_voting]\n",
    "\n",
    "\n",
    "X_voting_train = df_voting.values[:train_data.shape[0]]\n",
    "y_voting_train = train_data['Popularity'].values\n",
    "y_voting_train[y_voting_train==-1] = 0\n",
    "X_voting_test = df_voting.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_voting_train.shape)\n",
    "print(type(X_voting_train))\n",
    "print(X_voting_train)\n",
    "print(y_voting_train.shape)\n",
    "print(type(y_voting_train))\n",
    "print(y_voting_train)\n",
    "\n",
    "X_voting_train_split, X_voting_valid_split, y_voting_train_split, y_voting_valid_split = train_test_split(X_voting_train, y_voting_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To construct the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def training(X_train, y_train, clf):\n",
    "    score = cross_validate(clf, X_train, y_train, scoring='roc_auc', return_train_score=True, return_estimator=True)\n",
    "    print('train score: {:.6f} (+/-{:.6f})'.format(\n",
    "        np.mean(score['train_score']), np.std(score['train_score'])))\n",
    "    print('valid score: {:.6f} (+/-{:.6f})'.format(\n",
    "        np.mean(score['test_score']), np.std(score['test_score'])))\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return clf, np.mean(score['train_score']), np.mean(score['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To contruct the grid search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_cv(ct, X_train, y_train, clf, param_grid, verbose_=False):\n",
    "    X_train_ct = ct.fit_transform(X_train)\n",
    "    \n",
    "    # to report the grid search information\n",
    "    if(verbose_):\n",
    "        gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=-1, cv=5, return_train_score=True, verbose = 3)\n",
    "    else:\n",
    "        gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=-1, cv=5, return_train_score=True)\n",
    "        \n",
    "    gs.fit(X_train_ct, y_train)\n",
    "    \n",
    "    results, idx = gs.cv_results_, gs.best_index_\n",
    "    print('train score: {:.6f} (+/-{:.6f})'.format(results['mean_train_score'][idx], results['std_train_score'][idx]))\n",
    "    print('valid score: {:.6f} (+/-{:.6f})'.format(results['mean_test_score'][idx], results['std_test_score'][idx]))\n",
    "    print('best params:', gs.best_params_)\n",
    "    return gs.best_params_, gs.best_estimator_, results['mean_test_score'][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **- To set whether to run the grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_en = True\n",
    "grid_search_en = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - to store the best parameter to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_storage(dict_path, file_name, best_param, best_validation, remaining_dict, best_estimator_list=None):\n",
    "    if not os.path.exists(dict_path):\n",
    "        os.makedirs(dict_path)\n",
    "        \n",
    "    file_path = os.path.join(dict_path, file_name + \".txt\")\n",
    "    \n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(f'The features: {remaining_dict}\\n')\n",
    "        file.write(f'The best parameter: {best_param}\\n')\n",
    "        file.write(f'ngram_range : {ngram_range_}\\n')\n",
    "        if not (best_estimator_list==None):\n",
    "            file.write('The best estimator_list: ')\n",
    "            for i in range(len(best_estimator_list)):\n",
    "                file.write(f' {best_estimator_list[i][0]}')\n",
    "            file.write('\\n')\n",
    "        file.write(f'The best validation: {best_validation}\\n')\n",
    "        file.write('--------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-1-1. Grid sizing for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "xgb_model_path = '../output/best_models/xgboost/'\n",
    "if not os.path.exists(xgb_model_path):\n",
    "        os.makedirs(xgb_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.1s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.5s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.5s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.6s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.6s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.0s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.3s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.3s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.3s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.4s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.5s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.5s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.6s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.7s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.6s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.7s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.7s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.7s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.6s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.6s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.6s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.7s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.7s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.7s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.8s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.8s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.8s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.9s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.9s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  24.0s\n",
      "[CV] END gamma=1.1, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  24.2s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.3s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.4s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.6s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.1s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.3s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.4s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.3s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.5s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.4s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.3s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.5s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.3s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.4s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.6s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.6s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  23.5s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.1, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.7s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.5s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.2s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.3s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.6s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=97; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.7s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  20.5s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  22.7s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  22.8s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  22.7s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  23.2s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  22.9s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  22.8s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.0s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.0s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.0s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  23.1s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.3s\n",
      "[CV] END gamma=1.2, lambda=2.4, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  23.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  15.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  15.0s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=7, n_estimators=98; total time=  15.7s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  15.2s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  15.2s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  15.2s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  17.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  14.5s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  17.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  14.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  14.7s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  17.6s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=97; total time=  14.7s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  17.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  17.6s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  14.7s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  17.8s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  17.6s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=97; total time=  17.9s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  17.8s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.14, max_depth=8, n_estimators=98; total time=  17.8s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=7, n_estimators=98; total time=  15.0s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  16.2s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  16.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  16.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  16.4s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  16.5s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  16.6s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  16.5s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=97; total time=  16.9s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  16.8s\n",
      "[CV] END gamma=1.2, lambda=2.5, learning_rate=0.141, max_depth=8, n_estimators=98; total time=  17.0s\n",
      "train score: 0.783863 (+/-0.001916)\n",
      "valid score: 0.593022 (+/-0.004966)\n",
      "best params: {'gamma': 1.2, 'lambda': 2.5, 'learning_rate': 0.141, 'max_depth': 7, 'n_estimators': 97}\n"
     ]
    }
   ],
   "source": [
    "param_grid_xgb = {\n",
    "    'gamma' : [1.1, 1.2],\n",
    "    'lambda' : [2.4, 2.5],\n",
    "    'n_estimators': [97, 98],\n",
    "    'max_depth': [7, 8],\n",
    "    'learning_rate' : [0.14, 0.141]  \n",
    "}\n",
    "\n",
    "if (1):\n",
    "    best_xgb_param, best_xgb, best_xgb_valid = grid_search_cv(text_transformer, X_xgboost_train, y_xgboost_train, XGBClassifier(n_jobs=-1), param_grid_xgb, True)\n",
    "    xgb_path = xgb_model_path+'xgb_'+ str(round(best_xgb_valid, 6))+ '.pkl'\n",
    "    joblib.dump(best_xgb, xgb_path)\n",
    "    parameter_storage('../output/best_parameters', 'best_xgb_param', best_xgb_param, best_xgb_valid, remaining_dict_xgboost)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-1-2. Training for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model_path = xgb_model_path+'xgb_0.593022.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.783027 (+/-0.003699)\n",
      "valid score: 0.589135 (+/-0.005340)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_static_xgb = {\n",
    "    'gamma' : 1.2,\n",
    "    'lambda' : 2.5,\n",
    "    'n_estimators': 97,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate' : 0.141,\n",
    "    'n_jobs' : -1,\n",
    "    'random_state' : 0\n",
    "}\n",
    "\n",
    "clf_xgb = joblib.load(best_xgb_model_path)\n",
    "\n",
    "# if (grid_search_en):\n",
    "#     xgboost = Pipeline([('vect', text_transformer), ('clf', best_xgb)])\n",
    "# else :\n",
    "#     xgboost = Pipeline([('vect', text_transformer), ('clf', XGBClassifier(**param_grid_lgbm))])\n",
    "\n",
    "xgboost = Pipeline([('vect', text_transformer), ('clf', clf_xgb)])\n",
    "    \n",
    "_ = training(X_xgboost_train, y_xgboost_train, xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2-1. Grid sizing for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb_model_path = '../output/best_models/lgboost/'\n",
    "if not os.path.exists(lgb_model_path):\n",
    "        os.makedirs(lgb_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=regression; total time=   6.3s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=regression; total time=   6.4s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=poisson; total time=   6.8s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=poisson; total time=   6.8s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=poisson; total time=   6.8s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=regression; total time=   6.9s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=regression; total time=   6.9s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=regression; total time=   7.0s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=poisson; total time=   7.1s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=poisson; total time=   7.2s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=regression; total time=   7.2s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=regression; total time=   7.3s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=poisson; total time=   7.3s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=regression; total time=   7.3s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=regression; total time=   7.3s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=poisson; total time=   7.4s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=regression; total time=   7.4s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=poisson; total time=   7.4s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=poisson; total time=   7.5s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=poisson; total time=   7.5s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=poisson; total time=   7.5s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=regression; total time=   7.5s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=regression; total time=   7.5s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=poisson; total time=   7.6s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=poisson; total time=   7.6s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=regression; total time=   7.8s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=poisson; total time=   7.8s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=regression; total time=   7.8s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=poisson; total time=   7.8s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=poisson; total time=   7.9s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=regression; total time=   7.9s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=poisson; total time=   8.0s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=regression; total time=   8.2s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=regression; total time=   8.1s\n",
      "[CV] END learning_rate=0.0131, n_estimators=231, objective=regression; total time=   8.2s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=poisson; total time=   8.3s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=poisson; total time=   8.3s\n",
      "[CV] END learning_rate=0.13, n_estimators=230, objective=regression; total time=   7.9s\n",
      "[CV] END learning_rate=0.0131, n_estimators=230, objective=regression; total time=   8.4s\n",
      "[CV] END learning_rate=0.13, n_estimators=231, objective=poisson; total time=   8.6s\n",
      "train score: 0.699887 (+/-0.002702)\n",
      "valid score: 0.599386 (+/-0.007198)\n",
      "best params: {'learning_rate': 0.0131, 'n_estimators': 231, 'objective': 'regression'}\n"
     ]
    }
   ],
   "source": [
    "param_grid_lgbm = {\n",
    "    'learning_rate' : [0.13, 0.0131], \n",
    "    'n_estimators' : [230, 231],\n",
    "    'objective' : ['regression', 'poisson']\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_lgbm_param, best_lgbm, best_lgbm_valid = grid_search_cv(text_transformer, X_lightgbm_train, y_lightgbm_train, LGBMClassifier(n_jobs=-1, verbose=-1), param_grid_lgbm, True)\n",
    "    lgb_path = lgb_model_path+'lgb_'+ str(round(best_lgbm_valid, 6))+ '.pkl'\n",
    "    joblib.dump(best_lgbm, lgb_path)\n",
    "    parameter_storage('../output/best_parameters', 'best_lgbm_param', best_lgbm_param, best_lgbm_valid, remaining_dict_lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgb_model_path = lgb_model_path+'lgb_0.599386.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2-2. Training for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.779805 (+/-0.003876)\n",
      "valid score: 0.589276 (+/-0.008707)\n"
     ]
    }
   ],
   "source": [
    "params_LGBM = {\n",
    "    'random_state': 0, \n",
    "    'learning_rate' : 0.013,\n",
    "    'n_estimators' : 230,\n",
    "    'n_jobs' : -1,\n",
    "    'objective' : 'poisson'\n",
    "}\n",
    "\n",
    "# if (grid_search_en):\n",
    "#     lgbm = Pipeline([('vect', text_transformer), ('clf', best_lgbm)])\n",
    "# else :\n",
    "#     lgbm = Pipeline([('vect', text_transformer), ('clf', LGBMClassifier(**params_LGBM))])\n",
    "\n",
    "clf_lgb = joblib.load(best_lgb_model_path)\n",
    "\n",
    "lgbm = Pipeline([('vect', text_transformer), ('clf', clf_xgb)])\n",
    "\n",
    "_ = training(X_lightgbm_train, y_lightgbm_train, lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-3-1. Grid sizing for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_model_path = '../output/best_models/catboost/'\n",
    "if not os.path.exists(cat_model_path):\n",
    "        os.makedirs(cat_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ..................... (1 of 2) Processing pip1, total=  19.6s\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total=  21.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total=  21.7s\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.3min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.3min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.4min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.4min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 1.3min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.1min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 2.9min[Voting] ..................... (2 of 2) Processing pip2, total= 2.8min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.9min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 2.7min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.2min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.2min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 5.1min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.6min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.6min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.2min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 5.7min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.2min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.6min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.4min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.7min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.8min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.4min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.0min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.6min\n",
      "train score: nan (+/-nan)\n",
      "valid score: nan (+/-nan)\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.2min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.9min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 1.9min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.8min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.3min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.2min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 5.1min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.6min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.4min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 5.5min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 6.5min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.7min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.9min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.7min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.3min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.7min\n",
      "[1, 1] Finish!!\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.3min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.1min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.7min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.7min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.2min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.6min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.0min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.3min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.6min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 2.6min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.7min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 6.5min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.9min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.4min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.3min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.4min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.5min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 6.2min\n",
      "train score: 0.776913 (+/-0.003396)\n",
      "valid score: 0.591576 (+/-0.005620)\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.7min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 3.1min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 6.7min\n",
      "train score: 0.756916 (+/-0.002644)\n",
      "valid score: 0.595508 (+/-0.006460)\n",
      "train score: 0.746076 (+/-0.002426)\n",
      "valid score: 0.596427 (+/-0.006801)\n",
      "train score: 0.723156 (+/-0.002406)\n",
      "valid score: 0.597070 (+/-0.007498)\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.7min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.9min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.8min\n",
      "train score: 0.765801 (+/-0.002918)\n",
      "valid score: 0.594233 (+/-0.006131)\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.4min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.2min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 5.3min\n",
      "train score: 0.756916 (+/-0.002644)\n",
      "valid score: 0.595508 (+/-0.006460)\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 6.1min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.3min\n",
      "train score: 0.733098 (+/-0.002311)\n",
      "valid score: 0.596938 (+/-0.007217)\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 6.7min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.0min\n",
      "train score: 0.772973 (+/-0.003209)\n",
      "valid score: 0.592723 (+/-0.005800)\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 4.6min\n",
      "[5, 5] Finish!!\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 5.3min\n",
      "[5, 1] Finish!!\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.4min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 5.4min\n",
      "[1, 5] Finish!!\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 2.3min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 7.4min\n",
      "[3, 5] Finish!!\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total= 9.5min\n",
      "[3, 3] Finish!!\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total=16.0min\n",
      "[5, 3] Finish!!\n",
      "[CV 5/5] END depth=12, learning_rate=0.01, n_estimators=700;, score=(train=0.751, test=0.610) total time=47.5min\n",
      "[CV 2/5] END depth=12, learning_rate=0.01, n_estimators=700;, score=(train=0.752, test=0.598) total time=47.8min\n",
      "[CV 4/5] END depth=12, learning_rate=0.01, n_estimators=700;, score=(train=0.751, test=0.582) total time=47.8min\n",
      "[CV 1/5] END depth=12, learning_rate=0.01, n_estimators=700;, score=(train=0.751, test=0.606) total time=47.8min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total=15.7min\n",
      "[1, 3] Finish!!\n",
      "[CV 3/5] END depth=12, learning_rate=0.01, n_estimators=700;, score=(train=0.758, test=0.607) total time=47.9min\n",
      "[Voting] ..................... (2 of 2) Processing pip2, total=15.2min\n",
      "[3, 1] Finish!!\n",
      "train score: 0.752469 (+/-0.002700)\n",
      "valid score: 0.600565 (+/-0.010117)\n",
      "best params: {'depth': 12, 'learning_rate': 0.01, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "param_grid_catboost = {\n",
    "    'learning_rate' : [0.01], \n",
    "    'n_estimators' : [700],\n",
    "    'depth' : [12]\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_cat_params, best_cat_estimator, best_cat_valid= grid_search_cv(text_transformer, X_catboost_train, y_catboost_train, CatBoostClassifier(eval_metric='AUC',random_state=0, verbose=False), param_grid_catboost, True)\n",
    "    cat_path = cat_model_path+'cat_'+ str(round(best_cat_valid, 6))+ '.pkl'\n",
    "    joblib.dump(best_cat_estimator, cat_path)\n",
    "    parameter_storage('../output/best_parameters', 'best_cat_params', best_cat_params, best_cat_valid, remaining_dict_catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-3-2. Training for CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb Cell 64\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     catboost \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mct\u001b[39m\u001b[39m'\u001b[39m, text_transformer),(\u001b[39m'\u001b[39m\u001b[39mclf\u001b[39m\u001b[39m'\u001b[39m, CatBoostClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparamsCatBoost))])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m _ \u001b[39m=\u001b[39m training(X_catboost_train, y_catboost_train, catboost)\n",
      "\u001b[1;32m/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb Cell 64\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining\u001b[39m(X_train, y_train, clf):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     score \u001b[39m=\u001b[39m cross_validate(clf, X_train, y_train, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mroc_auc\u001b[39;49m\u001b[39m'\u001b[39;49m, return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_estimator\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain score: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m (+/-\u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         np\u001b[39m.\u001b[39mmean(score[\u001b[39m'\u001b[39m\u001b[39mtrain_score\u001b[39m\u001b[39m'\u001b[39m]), np\u001b[39m.\u001b[39mstd(score[\u001b[39m'\u001b[39m\u001b[39mtrain_score\u001b[39m\u001b[39m'\u001b[39m])))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mvalid score: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m (+/-\u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y240sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         np\u001b[39m.\u001b[39mmean(score[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m]), np\u001b[39m.\u001b[39mstd(score[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m])))\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/catboost/core.py:5100\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5097\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5098\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5100\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[1;32m   5101\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[1;32m   5102\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[1;32m   5103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/catboost/core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2315\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2317\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2320\u001b[0m         train_pool,\n\u001b[1;32m   2321\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2322\u001b[0m         params,\n\u001b[1;32m   2323\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2324\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2325\u001b[0m     )\n\u001b[1;32m   2327\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2328\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/catboost/core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1724\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paramsCatBoost = {\n",
    "    'eval_metric' : 'AUC',\n",
    "    'n_estimators' : 700,\n",
    "    'depth' : 12,\n",
    "    'learning_rate' : 0.01,\n",
    "    'random_state' : 0,\n",
    "    'verbose' : False\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    catboost = Pipeline([('ct', text_transformer),('clf', best_cat_estimator)])\n",
    "else :\n",
    "    catboost = Pipeline([('ct', text_transformer),('clf', CatBoostClassifier(**paramsCatBoost))])\n",
    "\n",
    "_ = training(X_catboost_train, y_catboost_train, catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_model_path = '../output/best_models/adaboost/'\n",
    "if not os.path.exists(ada_model_path):\n",
    "        os.makedirs(ada_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-4-1. Grid sizing for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 3/5] END estimator__max_depth=4, learning_rate=0.001, n_estimators=1500, random_state=0;, score=(train=0.628, test=0.594) total time= 8.2min\n",
      "[CV 2/5] END estimator__max_depth=4, learning_rate=0.001, n_estimators=1500, random_state=0;, score=(train=0.634, test=0.588) total time= 8.2min\n",
      "[CV 5/5] END estimator__max_depth=4, learning_rate=0.001, n_estimators=1500, random_state=0;, score=(train=0.627, test=0.602) total time= 8.2min\n",
      "[CV 1/5] END estimator__max_depth=4, learning_rate=0.001, n_estimators=1500, random_state=0;, score=(train=0.632, test=0.595) total time= 8.2min\n",
      "[CV 4/5] END estimator__max_depth=4, learning_rate=0.001, n_estimators=1500, random_state=0;, score=(train=0.628, test=0.576) total time= 8.3min\n",
      "train score: 0.629610 (+/-0.002879)\n",
      "valid score: 0.591157 (+/-0.008665)\n",
      "best params: {'estimator__max_depth': 4, 'learning_rate': 0.001, 'n_estimators': 1500, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_grid_AdaBoost = {\n",
    "    'estimator__max_depth' : [4],\n",
    "    'n_estimators': [1500],\n",
    "    'learning_rate': [0.001],\n",
    "    'random_state' : [0]\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_ada_params, best_ada_estimator, best_ada_valid = grid_search_cv(text_transformer, X_adaboost_train, y_adaboost_train,\n",
    "                                                         AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=0)), \n",
    "                                                         params_grid_AdaBoost, True)\n",
    "    ada_path = ada_model_path+'ada_'+ str(round(best_ada_valid, 6))+ '.pkl'\n",
    "    joblib.dump(best_ada_estimator, ada_path)\n",
    "    parameter_storage('../output/best_parameters', 'best_ada_params', best_ada_params, best_ada_valid, remaining_dict_adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ada_model_path = ada_model_path+'ada_0.591157.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-4-2. Training for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb Cell 70\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     adaBoost \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mvect\u001b[39m\u001b[39m'\u001b[39m, text_transformer), (\u001b[39m'\u001b[39m\u001b[39mclf\u001b[39m\u001b[39m'\u001b[39m, AdaBoostClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparam_Adaboost))])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m _ \u001b[39m=\u001b[39m training(X_adaboost_train, y_adaboost_train, adaBoost)\n",
      "\u001b[1;32m/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb Cell 70\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining\u001b[39m(X_train, y_train, clf):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     score \u001b[39m=\u001b[39m cross_validate(clf, X_train, y_train, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mroc_auc\u001b[39;49m\u001b[39m'\u001b[39;49m, return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_estimator\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain score: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m (+/-\u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         np\u001b[39m.\u001b[39mmean(score[\u001b[39m'\u001b[39m\u001b[39mtrain_score\u001b[39m\u001b[39m'\u001b[39m]), np\u001b[39m.\u001b[39mstd(score[\u001b[39m'\u001b[39m\u001b[39mtrain_score\u001b[39m\u001b[39m'\u001b[39m])))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mvalid score: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m (+/-\u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y246sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         np\u001b[39m.\u001b[39mmean(score[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m]), np\u001b[39m.\u001b[39mstd(score[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m])))\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:162\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    159\u001b[0m sample_weight[zero_weight_mask] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[39m# Boosting step\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_boost(\n\u001b[1;32m    163\u001b[0m     iboost, X, y, sample_weight, random_state\n\u001b[1;32m    164\u001b[0m )\n\u001b[1;32m    166\u001b[0m \u001b[39m# Early termination\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:569\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[39mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[39m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAMME.R\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_boost_real(iboost, X, y, sample_weight, random_state)\n\u001b[1;32m    571\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[1;32m    572\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:578\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[1;32m    576\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m--> 578\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[1;32m    580\u001b[0m y_predict_proba \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    582\u001b[0m \u001b[39mif\u001b[39;00m iboost \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_Adaboost = {\n",
    "    'estimator' : DecisionTreeClassifier(max_depth = 4), \n",
    "    'learning_rate' : 0.001, \n",
    "    'n_estimators' : 1560\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    adaBoost = Pipeline([('vect', text_transformer), ('clf', best_ada_estimator)])\n",
    "else :\n",
    "    adaBoost = Pipeline([('vect', text_transformer), ('clf', AdaBoostClassifier(**param_Adaboost))])\n",
    "    \n",
    "_ = training(X_adaboost_train, y_adaboost_train, adaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-5-1. Grid sizing for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_model_path = '../output/best_models/RF/'\n",
    "if not os.path.exists(RF_model_path):\n",
    "        os.makedirs(RF_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END max_depth=90, n_estimators=1150;, score=(train=0.998, test=0.589) total time= 2.6min\n",
      "[CV 1/5] END max_depth=90, n_estimators=1150;, score=(train=0.998, test=0.596) total time= 2.6min\n",
      "[CV 3/5] END max_depth=90, n_estimators=1150;, score=(train=0.999, test=0.589) total time= 2.6min\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 3/5] END max_depth=110, min_samples_leaf=1, n_estimators=1150;, score=(train=nan, test=nan) total time=   8.1s\n",
      "[CV 2/5] END max_depth=100, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   8.6s\n",
      "[CV 2/5] END max_depth=110, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   8.6s\n",
      "[CV 4/5] END max_depth=100, min_samples_leaf=1, n_estimators=1150;, score=(train=nan, test=nan) total time=   8.4s\n",
      "[CV 4/5] END max_depth=110, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   8.4s\n",
      "[CV 3/5] END max_depth=100, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   8.9s\n",
      "[CV 1/5] END max_depth=110, min_samples_leaf=1, n_estimators=1200;, score=(train=nan, test=nan) total time=   8.7s\n",
      "[CV 5/5] END max_depth=100, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   8.7s\n",
      "[CV 2/5] END max_depth=110, min_samples_leaf=1, n_estimators=1150;, score=(train=nan, test=nan) total time=   8.6s\n",
      "[CV 1/5] END max_depth=110, min_samples_leaf=1, n_estimators=1150;, score=(train=nan, test=nan) total time=   8.8s\n",
      "[CV 3/5] END max_depth=110, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   8.6s\n",
      "[CV 4/5] END max_depth=100, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   8.8s\n",
      "[CV 3/5] END max_depth=100, min_samples_leaf=1, n_estimators=1150;, score=(train=nan, test=nan) total time=   8.8s\n",
      "[CV 5/5] END max_depth=110, min_samples_leaf=1, n_estimators=1150;, score=(train=nan, test=nan) total time=   8.8s\n",
      "[CV 5/5] END max_depth=110, min_samples_leaf=1, n_estimators=1200;, score=(train=nan, test=nan) total time=   8.7s\n",
      "[CV 4/5] END max_depth=110, min_samples_leaf=1, n_estimators=1200;, score=(train=nan, test=nan) total time=   8.8s\n",
      "[CV 2/5] END max_depth=110, min_samples_leaf=1, n_estimators=1200;, score=(train=nan, test=nan) total time=   8.2s\n",
      "[CV 1/5] END max_depth=100, min_samples_leaf=1, n_estimators=1200;, score=(train=nan, test=nan) total time=   8.6s\n",
      "[CV 3/5] END max_depth=110, min_samples_leaf=1, n_estimators=1200;, score=(train=nan, test=nan) total time=   8.7s\n",
      "[CV 1/5] END max_depth=110, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   8.2s\n",
      "[CV 1/5] END max_depth=100, min_samples_leaf=1, n_estimators=1250;, score=(train=nan, test=nan) total time=   9.8s\n",
      "[CV 5/5] END max_depth=90, min_samples_leaf=1, n_estimators=1200;, score=(train=0.998, test=0.595) total time= 3.4min\n",
      "[CV 2/5] END max_depth=90, min_samples_leaf=1, n_estimators=1150;, score=(train=0.998, test=0.588) total time= 3.4min\n",
      "[CV 4/5] END max_depth=90, min_samples_leaf=1, n_estimators=1150;, score=(train=0.998, test=0.565) total time= 3.4min\n",
      "[CV 5/5] END max_depth=90, min_samples_leaf=1, n_estimators=1150;, score=(train=0.998, test=0.597) total time= 3.4min\n",
      "[CV 3/5] END max_depth=90, min_samples_leaf=1, n_estimators=1150;, score=(train=0.999, test=0.589) total time= 3.5min\n",
      "[CV 1/5] END max_depth=90, min_samples_leaf=1, n_estimators=1150;, score=(train=0.998, test=0.595) total time= 3.5min\n",
      "[CV 2/5] END max_depth=90, min_samples_leaf=1, n_estimators=1200;, score=(train=0.999, test=0.588) total time= 3.5min\n",
      "[CV 2/5] END max_depth=90, min_samples_leaf=1, n_estimators=1250;, score=(train=0.998, test=0.589) total time= 3.6min\n",
      "[CV 3/5] END max_depth=90, min_samples_leaf=1, n_estimators=1200;, score=(train=0.999, test=0.588) total time= 3.6min\n",
      "[CV 1/5] END max_depth=90, min_samples_leaf=1, n_estimators=1200;, score=(train=0.998, test=0.595) total time= 3.6min\n",
      "[CV 1/5] END max_depth=100, min_samples_leaf=1, n_estimators=1150;, score=(train=0.999, test=0.595) total time= 3.7min\n",
      "[CV 4/5] END max_depth=90, min_samples_leaf=1, n_estimators=1200;, score=(train=0.998, test=0.567) total time= 3.7min\n",
      "[CV 5/5] END max_depth=100, min_samples_leaf=1, n_estimators=1200;, score=(train=0.999, test=0.597) total time= 3.7min\n",
      "[CV 1/5] END max_depth=90, min_samples_leaf=1, n_estimators=1250;, score=(train=0.998, test=0.595) total time= 3.7min\n",
      "[CV 2/5] END max_depth=100, min_samples_leaf=1, n_estimators=1150;, score=(train=0.999, test=0.588) total time= 3.7min\n",
      "[CV 4/5] END max_depth=90, min_samples_leaf=1, n_estimators=1250;, score=(train=0.999, test=0.567) total time= 3.7min\n",
      "[CV 3/5] END max_depth=90, min_samples_leaf=1, n_estimators=1250;, score=(train=0.999, test=0.585) total time= 3.7min\n",
      "[CV 2/5] END max_depth=100, min_samples_leaf=1, n_estimators=1200;, score=(train=0.999, test=0.587) total time= 3.7min\n",
      "[CV 5/5] END max_depth=100, min_samples_leaf=1, n_estimators=1150;, score=(train=0.999, test=0.598) total time= 3.7min\n",
      "[CV 3/5] END max_depth=100, min_samples_leaf=1, n_estimators=1200;, score=(train=0.999, test=0.589) total time= 3.7min\n",
      "[CV 5/5] END max_depth=90, min_samples_leaf=1, n_estimators=1250;, score=(train=0.998, test=0.600) total time= 3.7min\n",
      "[CV 4/5] END max_depth=100, min_samples_leaf=1, n_estimators=1200;, score=(train=0.999, test=0.566) total time= 3.7min\n",
      "[CV 4/5] END max_depth=110, min_samples_leaf=1, n_estimators=1150;, score=(train=1.000, test=0.565) total time= 3.7min\n",
      "[CV 5/5] END max_depth=110, min_samples_leaf=1, n_estimators=1250;, score=(train=1.000, test=0.598) total time= 3.7min\n",
      "train score: 0.998407 (+/-0.000224)\n",
      "valid score: 0.587187 (+/-0.011168)\n",
      "best params: {'max_depth': 90, 'min_samples_leaf': 1, 'n_estimators': 1250}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_grid_RF = {\n",
    "    'n_estimators' : [1150, 1200, 1250],\n",
    "    'max_depth' : [90, 100, 110],\n",
    "    'min_samples_leaf' : [1]\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_RF_params, best_RF_estimator, best_RF_valid = grid_search_cv(text_transformer, X_RF_train, y_RF_train, RandomForestClassifier(n_jobs=-1), params_grid_RF, True)\n",
    "    RF_path = RF_model_path+'ada_'+ str(round(best_RF_valid, 6))+ '.pkl'\n",
    "    joblib.dump(best_RF_estimator, RF_path)\n",
    "    parameter_storage('../output/best_parameters', 'best_RF_params', best_RF_params, best_RF_valid, remaining_dict_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_RF_model_path = RF_model_path+'ada_0.587187.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-5-2. Training for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.998601 (+/-0.000162)\n",
      "valid score: 0.586302 (+/-0.009809)\n"
     ]
    }
   ],
   "source": [
    "param_RF = {\n",
    "    'n_jobs' : -1,\n",
    "    'random_state' : 0,\n",
    "    'n_estimators' : 1200,\n",
    "    'max_depth' : 100,\n",
    "    'min_samples_leaf': 1\n",
    "}\n",
    "\n",
    "# if (grid_search_en):\n",
    "#     RF = Pipeline([('vect', text_transformer), ('clf', best_RF_estimator)])\n",
    "# else :\n",
    "#     RF = Pipeline([('vect', text_transformer), ('clf', RandomForestClassifier(**param_RF))])\n",
    "\n",
    "clf_RF = joblib.load(best_RF_model_path)\n",
    "\n",
    "RF = Pipeline([('vect', text_transformer), ('clf', clf_RF)])\n",
    "\n",
    "_ = training(X_RF_train, y_RF_train, RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6. VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2-1. Grid sizing for Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('drop process', 'drop', [0, 2, 10, 11, 12]),\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [1]),\n",
    "        ('topic name process' , TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [3]),\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('drop process', 'drop', [0, 2, 10, 11, 12, 15, 16, 17, 18]),\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [1]),\n",
    "        ('topic name process' , TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [3])\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n",
    "cat_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('drop process', 'drop', [0, 2, 10, 11, 12]),\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [1]),\n",
    "        ('topic name process' , TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,1), lowercase=False), [3])\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n",
    "ada_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('drop process', 'drop', [0, 2, 10, 11, 12]),\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [1]),\n",
    "        ('topic name process' , TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [3])\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfc_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('drop process', 'drop', [0, 2, 10, 11, 12]),\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [1]),\n",
    "        ('topic name process' , TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [3])\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n",
    "voting_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [1]),\n",
    "        ('topic name process' , TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,2), lowercase=False), [3])\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_static_xgb = {\n",
    "    'gamma' : 1.2,\n",
    "    'lambda' : 2.5,\n",
    "    'n_estimators': 97,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate' : 0.141,\n",
    "    'n_jobs' : -1,\n",
    "    'random_state' : 0\n",
    "}\n",
    "\n",
    "params_static_LGBM = {\n",
    "    'random_state': 0, \n",
    "    'learning_rate' : 0.013,\n",
    "    'n_estimators' : 230,\n",
    "    'n_jobs' : -1,\n",
    "    'objective' : 'poisson'\n",
    "}\n",
    "\n",
    "params_static_cat = {\n",
    "    'eval_metric' : 'AUC',\n",
    "    'n_estimators' : 700,\n",
    "    'depth' : 12,\n",
    "    'learning_rate' : 0.01,\n",
    "    'random_state' : 0,\n",
    "    'verbose' : False\n",
    "}\n",
    "\n",
    "param_static_Adaboost = {\n",
    "    'estimator' : DecisionTreeClassifier(max_depth = 4), \n",
    "    'learning_rate' : 0.001, \n",
    "    'n_estimators' : 1560\n",
    "}\n",
    "\n",
    "param_static_RF = {\n",
    "    'n_jobs' : -1,\n",
    "    'random_state' : 0,\n",
    "    'n_estimators' : 1200,\n",
    "    'max_depth' : 100,\n",
    "    'min_samples_leaf': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb = Pipeline([('vect', xgb_transformer), ('clf', XGBClassifier(**param_static_xgb))])\n",
    "pipeline_lgb = Pipeline([('vect', lgb_transformer), ('clf',  LGBMClassifier(**params_static_LGBM))])\n",
    "pipeline_cat = Pipeline([('vect', cat_transformer), ('clf',  CatBoostClassifier(**params_static_cat))])\n",
    "pipeline_ada = Pipeline([('vect', ada_transformer), ('clf',  AdaBoostClassifier(**param_static_Adaboost))])\n",
    "pipeline_rfc = Pipeline([('vect', rfc_transformer), ('clf',  RandomForestClassifier(**param_static_RF))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_xgb = Pipeline([('vect', xgb_transformer), ('clf', joblib.load(best_xgb_model_path))])\n",
    "# pipeline_lgb = Pipeline([('vect', lgb_transformer), ('clf', joblib.load(best_lgb_model_path))])\n",
    "# pipeline_cat = Pipeline([('vect', cat_transformer), ('clf', joblib.load(best_xgb_model_path))])\n",
    "# pipeline_ada = Pipeline([('vect', ada_transformer), ('clf', joblib.load(best_ada_model_path))])\n",
    "# pipeline_rfc = Pipeline([('vect', rfc_transformer), ('clf', joblib.load(best_RF_model_path))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classifier = 3\n",
    "weight_range = 3\n",
    "# voting_estimator_list = [('pip1', pipeline_xgb), ('pip2', pipeline_lgb)]\n",
    "voting_estimator_list = [('pip1', pipeline_xgb), ('pip2', pipeline_lgb), ('pip3', pipeline_cat)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(voting_estimator_list) != num_classifier) :\n",
    "    print(\"Error: the numver of the classifier must equal to the estimator_list element number! Please check again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to find the weight combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def weight_list_generator(weight_range_, num_classifier_):   \n",
    "    weight_range = weight_range_\n",
    "    weight_list = []\n",
    "\n",
    "    binary_values = (1, 3, 5)\n",
    "    weight_list = [list(i) for i in list(itertools.product(binary_values, repeat=num_classifier_))]\n",
    "\n",
    "    print('weight list = ', weight_list)\n",
    "    print('length of weight list = ', len(weight_list))\n",
    "    return weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight list =  [[1, 1, 1], [1, 1, 3], [1, 1, 5], [1, 3, 1], [1, 3, 3], [1, 3, 5], [1, 5, 1], [1, 5, 3], [1, 5, 5], [3, 1, 1], [3, 1, 3], [3, 1, 5], [3, 3, 1], [3, 3, 3], [3, 3, 5], [3, 5, 1], [3, 5, 3], [3, 5, 5], [5, 1, 1], [5, 1, 3], [5, 1, 5], [5, 3, 1], [5, 3, 3], [5, 3, 5], [5, 5, 1], [5, 5, 3], [5, 5, 5]]\n",
      "length of weight list =  27\n"
     ]
    }
   ],
   "source": [
    "weight_list = weight_list_generator(weight_range_ = weight_range, \n",
    "                                    num_classifier_ = num_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* multi-thread grid search for voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_model_path = '../output/best_models/voting/'\n",
    "if not os.path.exists(voting_model_path):\n",
    "        os.makedirs(voting_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current weight = [1, 1]\n",
      "current weight = [1, 3]\n",
      "current weight = [1, 5]\n",
      "current weight = [3, 1]\n",
      "current weight = [3, 3]\n",
      "current weight = [3, 5]\n",
      "current weight = [5, 1]\n",
      "current weight = [5, 3]\n",
      "current weight = [5, 5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb Cell 83\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y300sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     mem\u001b[39m.\u001b[39mappend(thread)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y300sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m mem:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y300sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y300sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     mem\u001b[39m.\u001b[39mremove(thread)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text_Feature_Engineering.ipynb#Y300sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mend once\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1111\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1133\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.1min\n",
      "[Voting] ..................... (1 of 2) Processing pip1, total= 1.1min\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "param_grid_voting_static = {\n",
    "    'estimators' : voting_estimator_list,\n",
    "    'voting' : 'soft',\n",
    "    'flatten_transform' : True, \n",
    "    'verbose' : True\n",
    "}\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_weight(weight):\n",
    "    # share in multiple threads\n",
    "    global best_valid_score, best_weight, best_voting\n",
    "\n",
    "    voting = VotingClassifier(**param_grid_voting_static, weights=weight)\n",
    "    print('current weight =', weight)\n",
    "    clf_voting, _, valid_voting = training(X_voting_train, y_voting_train, voting)\n",
    "\n",
    "    # to protect the safety of shared variables\n",
    "    with lock:\n",
    "        if valid_voting > best_valid_score:\n",
    "            best_valid_score = valid_voting\n",
    "            best_weight = weight\n",
    "            best_voting = clf_voting\n",
    "    \n",
    "    print(f'{weight} Finish!!')\n",
    "    threadmax.release()\n",
    "        \n",
    "if (1):\n",
    "    best_valid_score = 0\n",
    "    best_weight = None\n",
    "    best_voting = None\n",
    "    mem = []\n",
    "    threadmax = threading.BoundedSemaphore(32)\n",
    "    \n",
    "    for weight in weight_list:\n",
    "        threadmax.acquire()\n",
    "        thread = threading.Thread(target=process_weight, args=(weight,))\n",
    "    \n",
    "        thread.start()\n",
    "        mem.append(thread)\n",
    "\n",
    "    for thread in mem:\n",
    "        thread.join()\n",
    "        mem.remove(thread)\n",
    "    \n",
    "    print('end once')\n",
    "\n",
    "    print('best_valid_score = %.6f' % best_valid_score)\n",
    "    print('best_weight = ', best_weight)\n",
    "    \n",
    "    parameter_storage('../output/best_parameters', 'best_weight', best_weight, best_valid_score, voting_estimator_list, remaining_dict_voting)\n",
    "\n",
    "voting_path = voting_model_path+'voting_'+ str(round(best_valid_score, 6))+ '.pkl'\n",
    "joblib.dump(best_voting, voting_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_voting_model_path = voting_model_path+'ada_0.587187.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ..................... (1 of 3) Processing pip1, total=  24.7s\n",
      "[Voting] ..................... (2 of 3) Processing pip2, total=  41.4s\n",
      "[Voting] ..................... (3 of 3) Processing pip3, total= 9.1min\n",
      "[Voting] ..................... (1 of 3) Processing pip1, total=   8.0s\n",
      "[Voting] ..................... (2 of 3) Processing pip2, total=  12.5s\n",
      "[Voting] ..................... (3 of 3) Processing pip3, total= 9.2min\n",
      "[Voting] ..................... (1 of 3) Processing pip1, total=   4.4s\n",
      "[Voting] ..................... (2 of 3) Processing pip2, total=   8.0s\n",
      "[Voting] ..................... (3 of 3) Processing pip3, total= 9.2min\n",
      "[Voting] ..................... (1 of 3) Processing pip1, total=   5.5s\n",
      "[Voting] ..................... (2 of 3) Processing pip2, total=  11.6s\n",
      "[Voting] ..................... (3 of 3) Processing pip3, total= 9.3min\n",
      "[Voting] ..................... (1 of 3) Processing pip1, total=   6.0s\n",
      "[Voting] ..................... (2 of 3) Processing pip2, total=  10.0s\n",
      "[Voting] ..................... (3 of 3) Processing pip3, total= 9.3min\n",
      "train score: 0.759963 (+/-0.002844)\n",
      "valid score: 0.601150 (+/-0.008420)\n",
      "[Voting] ..................... (1 of 3) Processing pip1, total=   6.5s\n",
      "[Voting] ..................... (2 of 3) Processing pip2, total=  11.5s\n",
      "[Voting] ..................... (3 of 3) Processing pip3, total= 9.9min\n"
     ]
    }
   ],
   "source": [
    "# no n_jobs\n",
    "prarms_voting = {\n",
    "    'estimators' : voting_estimator_list, \n",
    "    'voting' : 'soft',\n",
    "    'weights' : [1, 2, 3],\n",
    "    'flatten_transform' : True,\n",
    "    'verbose' : True\n",
    "}\n",
    "\n",
    "if (0):\n",
    "    voting = VotingClassifier(**param_grid_voting_static, weights=best_weight)\n",
    "else :\n",
    "    voting = VotingClassifier(**prarms_voting)\n",
    "\n",
    "_ = training(X_voting_train, y_voting_train, voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = voting\n",
    "\n",
    "y_score = best_model.predict_proba(X_voting_test)[:, 1]\n",
    "\n",
    "df_pred = pd.DataFrame({'Id': test_data['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('../output/test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
