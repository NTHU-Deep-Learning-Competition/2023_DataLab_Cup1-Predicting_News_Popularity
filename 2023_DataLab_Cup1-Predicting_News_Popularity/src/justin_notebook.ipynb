{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>DataLab Cup 1: Text Feature Engineering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. To load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../dataset/train.csv')\n",
    "test_data  = pd.read_csv('../dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11847, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
       "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
       "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
       "4   4          -1  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27643</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27644</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27645</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27646</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27647</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id                                       Page content\n",
       "0  27643  <html><head><div class=\"article-info\"><span cl...\n",
       "1  27644  <html><head><div class=\"article-info\"><span cl...\n",
       "2  27645  <html><head><div class=\"article-info\"><span cl...\n",
       "3  27646  <html><head><div class=\"article-info\"><span cl...\n",
       "4  27647  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "display(train_data.head())\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. To extract the features from the dataset\n",
    "\n",
    "將一些我們想要用到的feature從dataset中提取出來。以下為提取的特徵:\n",
    "\n",
    "- title\n",
    "- time(year/month/day/hour/minute/second)\n",
    "- number of images (num_img)\n",
    "- number of videos (num_video)\n",
    "- author name\n",
    "- topic\n",
    "- channel\n",
    "- length of content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "# to get the attribute of the 'title', 'year/month/date/day/hour/minute/second/is_weekend', 'num_img', 'num_video', 'author name', 'topic', 'channel', 'content length'\n",
    "\n",
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # 1. to find the 'title' (body > h1)\n",
    "    title = soup.find('h1').string.strip().lower()\n",
    "\n",
    "    # 2. to find time(body > div > span > time)\n",
    "\n",
    "    \n",
    "    date_string = soup.find('time')\n",
    "    try:\n",
    "        date_string = date_string['datetime']\n",
    "    except:\n",
    "        date_string = 'wed, 10 oct 2014 15:00:43 +0000'\n",
    "        \n",
    "    date_string = date_string.strip().lower()\n",
    "    datetimes = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S %z')\n",
    "    \n",
    "    \n",
    "    year = datetimes.year\n",
    "    month = datetimes.month\n",
    "    date = datetimes.day\n",
    "    day = pd.Timestamp(str(year)+'-'+str(month)+'-'+str(date)).dayofweek+1\n",
    "    is_weekend = 1 if (day==6 or day==7) else 0\n",
    "    hour = datetimes.hour\n",
    "    minute = datetimes.minute\n",
    "    second = datetimes.second\n",
    "    \n",
    "\n",
    "    # 3. to find the number of images\n",
    "    num_img  = len(soup.find_all('img'))\n",
    "    \n",
    "\n",
    "    # 4. to find the number of videos\n",
    "    num_video = len(soup.find_all('iframe'))\n",
    "    \n",
    "\n",
    "    # 5. to find the author name\n",
    "    article_info = soup.find('div', class_='article-info')\n",
    "    author = article_info.find('span', class_='author_name') or article_info.find('span', class_='byline basic')\n",
    "\n",
    "    if (author != None):\n",
    "        if (author.find('a') != None):\n",
    "            author = author.find('a')\n",
    "            author_name = author.get_text().lower()\n",
    "        else :\n",
    "            author_name = author.get_text().lower()\n",
    "    else :\n",
    "        author_name = 'not found'\n",
    "    \n",
    "    \n",
    "\n",
    "    # 6. to find the article topic\n",
    "    footer = soup.find('footer', class_='article-topics')\n",
    "    topic = footer.get_text().split(': ')[1]\n",
    "    \n",
    "    # 7. to find the channel\n",
    "    channel = soup.find('article')['data-channel'].strip().lower()\n",
    "    \n",
    "    '''\n",
    "    print('title = ', title, type(title))\n",
    "    print('time = ', year, \"/\", month, \"/\",day, \" \",hour, \":\",minute, \":\",second, type(year))\n",
    "    print('number of images = ', num_img, type(num_img))\n",
    "    print('number of videos = ', num_video, type(num_video))\n",
    "    print('author_name = ', author_name, type(author_name))\n",
    "    print('topic = ', topic, type(topic))\n",
    "    print('channel = ', channel, type(channel))\n",
    "    '''\n",
    "    \n",
    "    # 8. to find the content length\n",
    "    content = soup.body.find('section', class_='article-content').get_text()\n",
    "    len_content = len(content)\n",
    "\n",
    "    # print('topic = ', topic, type(topic))\n",
    "\n",
    "    return title, author_name, channel, topic, year, month, date, day, is_weekend, hour, minute, second, num_img, num_video, len_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m feature_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m content \u001b[39min\u001b[39;00m (train_data[\u001b[39m'\u001b[39m\u001b[39mPage content\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     feature_list\u001b[39m.\u001b[39mappend(preprocessor(content))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m content \u001b[39min\u001b[39;00m (test_data[\u001b[39m'\u001b[39m\u001b[39mPage content\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     feature_list\u001b[39m.\u001b[39mappend(preprocessor(content))\n",
      "\u001b[1;32m/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocessor\u001b[39m(text):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(text, \u001b[39m'\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# 1. to find the 'title' (body > h1)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254432d444c33227d/home/justinliu/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/justin_notebook.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     title \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mh1\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mstring\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[1;32m    336\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[1;32m    479\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/bs4/builder/_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[1;32m    379\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[1;32m    381\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    382\u001b[0m     \u001b[39m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[39m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39m# when there's an error in the doctype declaration.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m ParserRejectedMarkup(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/html/parser.py:111\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[0;32m--> 111\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/html/parser.py:173\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    171\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_starttag(i)\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m--> 173\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_endtag(i)\n\u001b[1;32m    174\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m<!--\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[1;32m    175\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_comment(i)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/html/parser.py:421\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[1;32m    419\u001b[0m         \u001b[39mreturn\u001b[39;00m gtpos\n\u001b[0;32m--> 421\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_endtag(elem)\n\u001b[1;32m    422\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_cdata_mode()\n\u001b[1;32m    423\u001b[0m \u001b[39mreturn\u001b[39;00m gtpos\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/bs4/builder/_htmlparser.py:176\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malready_closed_empty_element\u001b[39m.\u001b[39mremove(name)\n\u001b[1;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoup\u001b[39m.\u001b[39;49mhandle_endtag(name)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/bs4/__init__.py:770\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called by the tree builder when an ending tag is encountered.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \n\u001b[1;32m    766\u001b[0m \u001b[39m:param name: Name of the tag.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39m:param nsprefix: Namespace prefix for the tag.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39m#print(\"End tag: \" + name)\u001b[39;00m\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendData()\n\u001b[1;32m    771\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popToTag(name, nsprefix)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/bs4/__init__.py:617\u001b[0m, in \u001b[0;36mBeautifulSoup.endData\u001b[0;34m(self, containerClass)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    616\u001b[0m containerClass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstring_container(containerClass)\n\u001b[0;32m--> 617\u001b[0m o \u001b[39m=\u001b[39m containerClass(current_data)\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobject_was_parsed(o)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/bs4/element.py:955\u001b[0m, in \u001b[0;36mNavigableString.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     u \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, value, DEFAULT_OUTPUT_ENCODING)\n\u001b[0;32m--> 955\u001b[0m u\u001b[39m.\u001b[39;49msetup()\n\u001b[1;32m    956\u001b[0m \u001b[39mreturn\u001b[39;00m u\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.8/site-packages/bs4/element.py:189\u001b[0m, in \u001b[0;36mPageElement.setup\u001b[0;34m(self, parent, previous_element, next_element, previous_sibling, next_sibling)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_sibling \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_sibling\u001b[39m.\u001b[39mprevious_sibling \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mif\u001b[39;00m (previous_sibling \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mcontents):\n\u001b[1;32m    191\u001b[0m     previous_sibling \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mcontents[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_sibling \u001b[39m=\u001b[39m previous_sibling\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_list = []\n",
    "\n",
    "for content in (train_data['Page content']):\n",
    "    feature_list.append(preprocessor(content))\n",
    "\n",
    "for content in (test_data['Page content']):\n",
    "    feature_list.append(preprocessor(content))\n",
    "\n",
    "df_all = pd.DataFrame(\n",
    "        feature_list, \n",
    "        columns=['title', 'author_name', 'channel', 'topic', 'year', 'month', 'date', 'day', 'is_weekend', 'hour', 'minute', 'second', 'num_img', 'num_video', 'len_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 . Preprocessing - tokenization\n",
    "\n",
    "To split the text corpora into individual elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 . Preprocessing - Word Stemming\n",
    "\n",
    "There are two ways of word stemming\n",
    "\n",
    "1. PorterStemmer(Stemming): break the word in rule-besed way, which will lead to the probelm of overstemming\n",
    "\n",
    "2. WordNetLemmatizer(Lemmatization): Stem the words will better performance, while time-consuming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def word_stemming(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    lm = WordNetLemmatizer()\n",
    "    words = re.split('\\s', text.strip())\n",
    "    lemmatized_words = [lm.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "print(word_stemming('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "print(tokenizer_stem('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 Preprocessing - Stop-Word Removal\n",
    "\n",
    "儘管刪除停用詞在某些情況下（例如 BoW 和特徵哈希）可以有益於簡化表示，並可能提高文字分析的準確性，但並不總是必要，特別是在使用 TF-IDF 時。是否刪除停用詞應基於文本分析任務的具體要求以及資料集的特性來進行決策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def stop_word_removal(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(stop_word_removal('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-4 Create TF-IDF feature representation ([ref](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "text_transformer = ColumnTransformer(\n",
    "    [\n",
    "        #('title preprocess', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [0]),            # to process the title paragraph\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [0]),\n",
    "        #('channel process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [1]),\n",
    "        ('topic name process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [1]),\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "catboost_transformer =  ColumnTransformer(\n",
    "    [\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [0]),            # to process the title paragraph\n",
    "        ('topic name process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [1])\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False)\n",
    "tfidf.fit(df_all['title'])\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names_out()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(df_all['title']).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones((1, tfidf_sum.shape[0])))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model training\n",
    "\n",
    "- XGBoost\n",
    "\n",
    "- LightGBM\n",
    "\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.loc[:, [\n",
    "                    #'title', \n",
    "                    'author_name', \n",
    "                    #'channel', \n",
    "                    'topic', \n",
    "                    'year', \n",
    "                    'month',\n",
    "                    'date', \n",
    "                    'day', \n",
    "                    'is_weekend',\n",
    "                    'hour', \n",
    "                    # 'minute', \n",
    "                    # 'second', \n",
    "                    # 'num_img', \n",
    "                    'num_video', \n",
    "                    'len_content'\n",
    "                    ]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = df.values[:train_data.shape[0]]\n",
    "y_train = train_data['Popularity'].values\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = df.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(type(X_train))\n",
    "print(X_train)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To construct the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def training(clf):\n",
    "    clf_cv = cross_validate(clf, X_train, y_train, scoring='roc_auc', return_train_score=True, return_estimator=True)\n",
    "    print('train score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(clf_cv['train_score']), np.std(clf_cv['train_score'])))\n",
    "    print('valid score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(clf_cv['test_score']), np.std(clf_cv['test_score'])))\n",
    "\n",
    "    clf.fit(X_train_split, y_train_split)\n",
    "    train_score = roc_auc_score(\n",
    "        y_train_split, clf.predict_proba(X_train_split)[:, 1])\n",
    "    valid_score = (roc_auc_score(\n",
    "        y_valid_split, clf.predict_proba(X_valid_split)[:, 1]))\n",
    "    \n",
    "    return clf, np.mean(clf_cv['train_score']),  np.mean(clf_cv['test_score'])\n",
    "    # return clf, train_score, valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1-1. Grid Sizing for XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'gamma' : [0, 0.5, 1, 1.5, 2],\n",
    "    'lambda' : [1.5, 2, 2.5, 3],\n",
    "    'n_estimators': [100, 120, 140, 160, 180],\n",
    "    'max_depth': [6, 8, 10, 12, 14],\n",
    "    'learning_rate' : [0.14, 0.15, 0.16]  \n",
    "}\n",
    "\n",
    "best_xgb_param, best_xgb = grid_search_cv(text_transformer, XGBClassifier(n_jobs=-1), param_grid_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1-2. Training for XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'gamma' : 1,\n",
    "    'lambda' : 2.5,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate' : 0.14,\n",
    "    'n_jobs' : -1\n",
    "}\n",
    "\n",
    "# '**' 是一種解包（unpacking）操作符，它可以用於將字典中的鍵值對以關鍵字參數的方式傳遞給函數或方法\n",
    "xgboost = Pipeline([('vect', text_transformer),\n",
    "                    ('clf', XGBClassifier(**param_grid_lgbm))])\n",
    "\n",
    "training(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3 CatBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-1 Use simple for-loop to tuning the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# catboost = Pipeline([('ct', catboost_transformer),\n",
    "#                      ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=290, learning_rate=0.06))])\n",
    "# catboost, train_error, valid_error = training(catboost)\n",
    "\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "with open('../catboost_log/tree_num_tree_depth_vs_lr.log','w') as file:\n",
    "    for tree_num in range(100,0,100):\n",
    "        for tree_depth in range(3, 11): # depth from 3 to 10 (8)\n",
    "            for i in range(1,11): # lr from 0.01 to 0.10 (10)\n",
    "                print (\"tree_num: \", tree_num, \"tree_depth :\", tree_depth, \" learning rate: \", i*0.01)\n",
    "                catboost = Pipeline([('ct', catboost_transformer),\n",
    "                                    ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=tree_num, depth = tree_depth, learning_rate=i*0.01, random_state = 0))])\n",
    "                catboost, train_acc, valid_acc = training(catboost)\n",
    "                train_acc_list.append(train_acc)\n",
    "                valid_acc_list.append(valid_acc)\n",
    "                write_str = \"tree_num: \" + str(tree_num) + \" tree_depth:\"+ str(tree_depth) + \" learning_rate: \" +  str(i*0.01) + \" train_acc: \" + str(train_acc) + \" valid_acc: \" + str(valid_acc) + '\\n'\n",
    "                file.write (write_str)\n",
    "                file.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-2 Use matplot to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# train_acc_list = []\n",
    "# valid_acc_list = []\n",
    "\n",
    "# with open ('../catboost_log/tree_num_tree_depth_vs_lr.log','r') as file:\n",
    "#     for line in file:\n",
    "#         if 'train_acc' in line and 'valid_acc' in line:\n",
    "#             train_acc = float(line.split('train_acc: ')[1].split(' valid_acc: ')[0])\n",
    "#             valid_acc = float(line.split('valid_acc: ')[1])\n",
    "#             train_acc_list.append(train_acc)\n",
    "#             valid_acc_list.append(valid_acc)\n",
    "\n",
    "# print(\"Train Accuracy List: \", train_acc_list[:5])\n",
    "# print(\"Valid Accuracy List: \", valid_acc_list[:5])\n",
    "\n",
    "# tree_num_list = list(range(100,1100,100))\n",
    "# tree_depth_list = list(range(3,11))\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# best_result_list = []\n",
    "# for i in range(5):\n",
    "#     tree_num_train_list = train_acc_list[i*80:(i+1)*80] # there are 80 data in each list\n",
    "#     tree_num_valid_list = valid_acc_list[i*80:(i+1)*80]\n",
    "#     max_valid_acc = max(tree_num_valid_list)\n",
    "#     max_index = tree_num_valid_list.index(max_valid_acc) # the index should be 0 ~ 79\n",
    "#     train_acc = tree_num_train_list[max_index]\n",
    "#     max_lr = (max_index % 10) * 0.01\n",
    "#     max_tree_depth  = (max_index-1) // 10 + 3\n",
    "#     best_result_list.append((max_tree_depth, max_lr, max_valid_acc))\n",
    "#     print(\"In tree num: \", str((i+1)*100), \", the best param (tree_depth, max_lr) is: (\", max_tree_depth,\", \" ,max_lr, \"), and its train/valid acc = \",train_acc,'/', max_valid_acc)\n",
    "\n",
    "\n",
    "#     # plt.subplot(5,2,i+1)\n",
    "#     # # 繪製圖形\n",
    "#     # plt.plot(tree_depth_list, train_iter_300_list, marker='o', linestyle='--', color='b', label='Train acc')\n",
    "#     # plt.plot(tree_depth_list, valid_iter_300_list, marker='o', linestyle='--', color='r', label='Valid acc')\n",
    "\n",
    "#     # # 添加標籤和標題\n",
    "#     # plt.xlabel('tree_depth')\n",
    "#     # plt.ylabel('Accuracy')\n",
    "#     # title_name = 'Training and Validation Accuracy vs tree_depth on tree number = ' + str((i+1)*100)\n",
    "#     # plt.title(title_name)\n",
    "#     # plt.legend()\n",
    "\n",
    "# # 顯示圖形\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-3 Use grid search to select best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_cv(ct, clf, param_grid):\n",
    "    X_train_ct = ct.fit_transform(X_train)\n",
    "    gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=-1, cv=5, return_train_score=True)\n",
    "    gs.fit(X_train_ct, y_train)\n",
    "    results, idx = gs.cv_results_, gs.best_index_\n",
    "    print('train score: {:.5f} (+/-{:.5f})'.format(results['mean_train_score'][idx], results['std_train_score'][idx]))\n",
    "    print('valid score: {:.5f} (+/-{:.5f})'.format(results['mean_test_score'][idx], results['std_test_score'][idx]))\n",
    "    print('best params:', gs.best_params_)\n",
    "    return gs.best_params_, gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# param_grid_catboost = {\n",
    "#     'learning_rate' : [0.001, 0.005, 0.01, 0.05, 0.06 , 0.09, 0.1, 0.15, 0.2], \n",
    "#     'n_estimators' : [300, 350, 400, 450, 500, 550, 600,650,700,800,900,1000],\n",
    "#     'depth': [3,4,5,6,7,8,9,10,11,12,13,14,15,16],\n",
    "# }\n",
    "\n",
    "# best_cat_params, best_estimator_ = grid_search_cv(text_transformer, CatBoostClassifier( eval_metric='AUC',random_state=0 ), param_grid_catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3-4 Reproduce experiment\n",
    "\n",
    "By above code, we found that some result looks good:\n",
    "\n",
    "+ In tree num:  100 , the best param (tree_depth, max_lr) is: ( 10 ,  0.09 ), and its train/valid acc =  0.7094969129942881 / 0.5863367841056331\n",
    "+ In tree num:  200 , the best param (tree_depth, max_lr) is: ( 9 ,  0.08 ), and its train/valid acc =  0.7577161684304449 / 0.5857340744507373\n",
    "+ In tree num:  300 , the best param (tree_depth, max_lr) is: ( 10 ,  0.06 ), and its train/valid acc =  0.8133145020810143 / 0.5892105812052466\n",
    "+ In tree num:  400 , the best param (tree_depth, max_lr) is: ( 9 ,  0.09 ), and its train/valid acc =  0.8628525546978256 / 0.5877459417855228\n",
    "+ In tree num:  500 , the best param (tree_depth, max_lr) is: ( 9 ,  0.09 ), and its train/valid acc =  0.8918235409307705 / 0.587528463833626\n",
    "\n",
    "In below code, we want to reproduce this result by same setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# # 设置要忽略的警告\n",
    "# warnings.filterwarnings(\"ignore\", message=\"The parameter 'token_pattern' will not be used since 'tokenizer' is not None\")\n",
    "\n",
    "\n",
    "# catboost = Pipeline([('ct', catboost_transformer),\n",
    "#                                     ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=100, depth = 10, learning_rate=0.09, random_state = 0))])\n",
    "# catboost, train_acc, valid_acc = training(catboost)\n",
    "\n",
    "# print(train_acc, valid_acc)\n",
    "\n",
    "# catboost = Pipeline([('ct', catboost_transformer),\n",
    "#                                     ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=200, depth = 9, learning_rate=0.08, random_state = 0))])\n",
    "# catboost, train_acc, valid_acc = training(catboost)\n",
    "# print(train_acc, valid_acc)\n",
    "\n",
    "# catboost = Pipeline([('ct', catboost_transformer),\n",
    "#                                     ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=300, depth = 10, learning_rate=0.06, random_state = 0))])\n",
    "# catboost, train_acc, valid_acc = training(catboost)\n",
    "# print(train_acc, valid_acc)\n",
    "\n",
    "# catboost = Pipeline([('ct', catboost_transformer),\n",
    "#                                     ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=400, depth = 9, learning_rate=0.09, random_state = 0))])\n",
    "# catboost, train_acc, valid_acc = training(catboost)\n",
    "# print(train_acc, valid_acc)\n",
    "\n",
    "# catboost = Pipeline([('ct', catboost_transformer),\n",
    "#                                     ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=500, depth = 9, learning_rate=0.09, random_state = 0))])\n",
    "# catboost, train_acc, valid_acc = training(catboost)\n",
    "# print(train_acc, valid_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use specified param to get the y_pred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost = Pipeline([('ct', catboost_transformer),\n",
    "                                    ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=300, depth = 10, learning_rate=0.06, random_state = 0))])\n",
    "catboost, train_acc, valid_acc = training(catboost)\n",
    "\n",
    "y_score = catboost.predict_proba(X_test)[:, 1]\n",
    "df_pred = pd.DataFrame({'Id': test_data['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('../output/test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
