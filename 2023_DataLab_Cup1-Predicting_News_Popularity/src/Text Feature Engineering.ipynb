{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>DataLab Cup 1: Text Feature Engineering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. To load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../dataset/train.csv')\n",
    "test_data  = pd.read_csv('../dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
       "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
       "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
       "4   4          -1  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. To extract the features from the dataset\n",
    "\n",
    "將一些我們想要用到的feature從dataset中提取出來。以下為提取的特徵:\n",
    "\n",
    "- title\n",
    "- time(year/month/day/hour/minute/second)\n",
    "- number of images (num_img)\n",
    "- number of videos (num_video)\n",
    "- author name\n",
    "- topic\n",
    "- channel\n",
    "- length of content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"nasa's grand challenge: stop asteroids from destroying earth\",\n",
       " 'clara moskowitz',\n",
       " 'world',\n",
       " 'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ',\n",
       " 2013,\n",
       " 6,\n",
       " 19,\n",
       " 3,\n",
       " 0,\n",
       " 15,\n",
       " 4,\n",
       " 30,\n",
       " 1,\n",
       " 0,\n",
       " 3591)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "# to get the attribute of the 'title', 'year/month/date/day/hour/minute/second/is_weekend', 'num_img', 'num_video', 'author name', 'topic', 'channel', 'content length'\n",
    "\n",
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # 1. to find the 'title' (body > h1)\n",
    "    title = soup.find('h1').string.strip().lower()\n",
    "\n",
    "    # 2. to find time(body > div > span > time)\n",
    "    date_string = soup.find('time')['datetime'].strip().lower()\n",
    "    datetimes = datetime.strptime(date_string, \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "\n",
    "    year = datetimes.year\n",
    "    month = datetimes.month\n",
    "    date = datetimes.day\n",
    "    day = pd.Timestamp(str(year)+'-'+str(month)+'-'+str(date)).dayofweek+1\n",
    "    is_weekend = 1 if (day==6 or day==7) else 0\n",
    "    hour = datetimes.hour\n",
    "    minute = datetimes.minute\n",
    "    second = datetimes.second\n",
    "    \n",
    "\n",
    "    # 3. to find the number of images\n",
    "    num_img  = len(soup.find_all('img'))\n",
    "    \n",
    "\n",
    "    # 4. to find the number of videos\n",
    "    num_video = len(soup.find_all('iframe'))\n",
    "    \n",
    "\n",
    "    # 5. to find the author name\n",
    "    article_info = soup.find('div', class_='article-info')\n",
    "    author = article_info.find('span', class_='author_name') or article_info.find('span', class_='byline basic')\n",
    "\n",
    "    if (author != None):\n",
    "        if (author.find('a') != None):\n",
    "            author = author.find('a')\n",
    "            author_name = author.get_text().lower()\n",
    "        else :\n",
    "            author_name = author.get_text().lower()\n",
    "    else :\n",
    "        author_name = 'not found'\n",
    "    \n",
    "    \n",
    "\n",
    "    # 6. to find the article topic\n",
    "    footer = soup.find('footer', class_='article-topics')\n",
    "    topic = footer.get_text().split(': ')[1]\n",
    "    \n",
    "    # 7. to find the channel\n",
    "    channel = soup.find('article')['data-channel'].strip().lower()\n",
    "    \n",
    "    '''\n",
    "    print('title = ', title, type(title))\n",
    "    print('time = ', year, \"/\", month, \"/\",day, \" \",hour, \":\",minute, \":\",second, type(year))\n",
    "    print('number of images = ', num_img, type(num_img))\n",
    "    print('number of videos = ', num_video, type(num_video))\n",
    "    print('author_name = ', author_name, type(author_name))\n",
    "    print('topic = ', topic, type(topic))\n",
    "    print('channel = ', channel, type(channel))\n",
    "    '''\n",
    "    \n",
    "    # 8. to find the content length\n",
    "    content = soup.body.find('section', class_='article-content').get_text()\n",
    "    len_content = len(content)\n",
    "\n",
    "    # print('topic = ', topic, type(topic))\n",
    "\n",
    "    return title, author_name, channel, topic, year, month, date, day, is_weekend, hour, minute, second, num_img, num_video, len_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor content in (test_data['Page content']):\\n    feature_test_list.append(preprocessor(content))\\n\\ndf_test_all = pd.DataFrame(\\n        feature_test_list, \\n        columns=['title', 'author_name', 'channel', 'topic', 'year', 'month', 'date', 'day', 'is_weekend', 'hour', 'minute', 'second', 'num_img', 'num_video', 'len_content'])\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train_list = []\n",
    "feature_test_list = []\n",
    "\n",
    "for content in (train_data['Page content']):\n",
    "    feature_train_list.append(preprocessor(content))\n",
    "\n",
    "df_train_all = pd.DataFrame(\n",
    "        feature_train_list, \n",
    "        columns=['title', 'author_name', 'channel', 'topic', 'year', 'month', 'date', 'day', 'is_weekend', 'hour', 'minute', 'second', 'num_img', 'num_video', 'len_content'])\n",
    "\n",
    "'''\n",
    "for content in (test_data['Page content']):\n",
    "    feature_test_list.append(preprocessor(content))\n",
    "\n",
    "df_test_all = pd.DataFrame(\n",
    "        feature_test_list, \n",
    "        columns=['title', 'author_name', 'channel', 'topic', 'year', 'month', 'date', 'day', 'is_weekend', 'hour', 'minute', 'second', 'num_img', 'num_video', 'len_content'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 . Preprocessing - tokenization\n",
    "\n",
    "To split the text corpora into individual elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 . Preprocessing - Word Stemming\n",
    "\n",
    "There are two ways of word stemming\n",
    "\n",
    "1. PorterStemmer(Stemming): break the word in rule-besed way, which will lead to the probelm of overstemming\n",
    "\n",
    "2. WordNetLemmatizer(Lemmatization): Stem the words will better performance, while time-consuming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/student/mr111//mfhsieh22/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /users/student/mr111//mfhsieh22/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university,', 'universal,', 'university']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.data.path.append('/home/mfhsieh/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def word_stemming(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    lm = WordNetLemmatizer()\n",
    "    words = re.split('\\s', text.strip())\n",
    "    lemmatized_words = [lm.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "print(word_stemming('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university,', 'universal,', 'univers']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "print(tokenizer_stem('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 Preprocessing - Stop-Word Removal\n",
    "\n",
    "儘管刪除停用詞在某些情況下（例如 BoW 和特徵哈希）可以有益於簡化表示，並可能提高文字分析的準確性，但並不總是必要，特別是在使用 TF-IDF 時。是否刪除停用詞應基於文本分析任務的具體要求以及資料集的特性來進行決策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_removal(text):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-4 Create TF-IDF feature representation ([ref](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "text_transformer = ColumnTransformer(\n",
    "    [\n",
    "        #('title preprocess', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [0]),            # to process the title paragraph\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,1), lowercase=False), [0]),\n",
    "        #('channel process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [1]),\n",
    "        ('topic name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=(1,1), lowercase=False), [1]),\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "to: 2.53\n",
      "the: 2.56\n",
      "in: 2.96\n",
      "a: 3.04\n",
      "of: 3.06\n",
      "for: 3.11\n",
      "and: 3.44\n",
      "is: 3.50\n",
      "on: 3.53\n",
      "your: 3.62\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "the: 815.311294761899\n",
      "to: 779.2825126570024\n",
      "a: 560.246222897993\n",
      "in: 554.0700384430594\n",
      "of: 530.6444224763436\n",
      "for: 516.0574390632206\n",
      "and: 393.9149074133082\n",
      "is: 386.79499423625896\n",
      "your: 381.58405078926234\n",
      "you: 380.68545839235753\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False)\n",
    "tfidf.fit(df_train_all['title'])\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names_out()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(df_train_all['title']).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones((1, tfidf_sum.shape[0])))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model training\n",
    "\n",
    "- XGBoost\n",
    "\n",
    "- LightGBM\n",
    "\n",
    "- CatBoost\n",
    "\n",
    "- AdaBoost\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>topic</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour</th>\n",
       "      <th>num_video</th>\n",
       "      <th>len_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clara moskowitz</td>\n",
       "      <td>Asteroid, Asteroids, challenge, Earth, Space, ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>christina warren</td>\n",
       "      <td>Apps and Software, Google, open source, opn pl...</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>Entertainment, NFL, NFL Draft, Sports, Televis...</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>Sports, Video, Videos, Watercooler</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connor finnegan</td>\n",
       "      <td>Entertainment, instagram, instagram video, NFL...</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_name                                              topic  year  \\\n",
       "0   clara moskowitz  Asteroid, Asteroids, challenge, Earth, Space, ...  2013   \n",
       "1  christina warren  Apps and Software, Google, open source, opn pl...  2013   \n",
       "2         sam laird  Entertainment, NFL, NFL Draft, Sports, Televis...  2014   \n",
       "3         sam laird                Sports, Video, Videos, Watercooler   2013   \n",
       "4   connor finnegan  Entertainment, instagram, instagram video, NFL...  2014   \n",
       "\n",
       "   month  date  day  is_weekend  hour  num_video  len_content  \n",
       "0      6    19    3           0    15          0         3591  \n",
       "1      3    28    4           0    17          0         1843  \n",
       "2      5     7    3           0    19         25         6646  \n",
       "3     10    11    5           0     2         21         1821  \n",
       "4      4    17    4           0     3          1         8919  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_train_all.loc[:, [\n",
    "                    #'title', \n",
    "                    'author_name', \n",
    "                    #'channel', \n",
    "                    'topic', \n",
    "                    'year', \n",
    "                    'month',\n",
    "                    'date', \n",
    "                    'day', \n",
    "                    'is_weekend',\n",
    "                    'hour', \n",
    "                    # 'minute', \n",
    "                    # 'second', \n",
    "                    # 'num_img', \n",
    "                    'num_video', \n",
    "                    'len_content'\n",
    "                    ]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "[['clara moskowitz'\n",
      "  'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ' 2013 ...\n",
      "  15 0 3591]\n",
      " ['christina warren'\n",
      "  'Apps and Software, Google, open source, opn pledge, patent lawsuit theater, software patents, Tech, U.S. '\n",
      "  2013 ... 17 0 1843]\n",
      " ['sam laird' 'Entertainment, NFL, NFL Draft, Sports, Television ' 2014\n",
      "  ... 19 25 6646]\n",
      " ...\n",
      " ['christine erickson' 'Food, hot dogs, humor, Photography, Watercooler '\n",
      "  2014 ... 12 0 1274]\n",
      " ['seth fiegerman' 'Business, marissa mayer, Media, stocks, Yahoo ' 2013\n",
      "  ... 20 0 2657]\n",
      " ['megan ranney' 'austin, Business, CurioCity, Small Business, Startups '\n",
      "  2014 ... 18 0 3027]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = df.values\n",
    "y_train = train_data['Popularity'].values\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = df.values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(type(X_train))\n",
    "print(X_train)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "print(y_train)\n",
    "\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To construct the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def training(clf):\n",
    "    clf_cv = cross_validate(clf, X_train, y_train, scoring='roc_auc', return_train_score=True, return_estimator=True)\n",
    "    print('train score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(clf_cv['train_score']), np.std(clf_cv['train_score'])))\n",
    "    print('valid score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(clf_cv['test_score']), np.std(clf_cv['test_score'])))\n",
    "\n",
    "    clf.fit(X_train_split, y_train_split)\n",
    "    print('train score: {:.5f}'.format(roc_auc_score(\n",
    "        y_train_split, clf.predict_proba(X_train_split)[:, 1])))\n",
    "    print('valid score: {:.5f}'.format(roc_auc_score(\n",
    "        y_valid_split, clf.predict_proba(X_valid_split)[:, 1])))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -To contruct the grid search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_cv(ct, clf, param_grid):\n",
    "    X_train_ct = ct.fit_transform(X_train)\n",
    "    X_valid_ct = ct.transform(X_valid_split)\n",
    "    \n",
    "    gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "    gs.fit(X_train_ct, y_train)\n",
    "    print('best params', gs.best_params_)\n",
    "    print('roc_auc-train = %.3f, roc_auc-valid = %.3f' % (gs.score(X_train_ct, y_train), gs.score(X_valid_ct, y_valid_split)))\n",
    "    return gs.best_params_, gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-1-1. Grid sizing for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text Feature Engineering.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m param_grid_xgb \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m0\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1.5\u001b[39m, \u001b[39m2\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlambda\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m1.5\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2.5\u001b[39m, \u001b[39m3\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m : [\u001b[39m0.14\u001b[39m, \u001b[39m0.15\u001b[39m, \u001b[39m0.16\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m best_xgb_param, best_xgb \u001b[39m=\u001b[39m grid_search_cv(text_transformer, XGBClassifier(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), param_grid_xgb)\n",
      "\u001b[1;32m/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text Feature Engineering.ipynb Cell 30\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrid_search_cv\u001b[39m(ct, clf, param_grid):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     X_train_ct \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     X_valid_ct \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39mtransform(X_valid_split)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     gs \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mclf, param_grid\u001b[39m=\u001b[39mparam_grid, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mroc_auc\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:727\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    725\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m--> 727\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, y, _fit_transform_one)\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[1;32m    730\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:658\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    652\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m    653\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[1;32m    654\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m )\n\u001b[1;32m    657\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    659\u001b[0m         delayed(func)(\n\u001b[1;32m    660\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[1;32m    661\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    662\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    663\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m    664\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    665\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    670\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'gamma' : [0, 0.5, 1, 1.5, 2],\n",
    "    'lambda' : [1.5, 2, 2.5, 3],\n",
    "    'n_estimators': [100, 120, 140, 160, 180],\n",
    "    'max_depth': [6, 8, 10, 12, 14],\n",
    "    'learning_rate' : [0.14, 0.15, 0.16]\n",
    "}\n",
    "\n",
    "best_xgb_param, best_xgb = grid_search_cv(text_transformer, XGBClassifier(n_jobs=-1), param_grid_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-1-2. Training for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.81234 (+/-0.00367)\n",
      "valid score: 0.58892 (+/-0.00724)\n",
      "train score: 0.82065\n",
      "valid score: 0.58606\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1, remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;author name process&#x27;,\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                                  [0]),\n",
       "                                                 (&#x27;topic name process&#x27;,\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                                  [1])])),\n",
       "                (&#x27;...\n",
       "                               feature_types=None, gamma=1, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=None,\n",
       "                               interaction_constraints=None, lambda=2.5,\n",
       "                               learning_rate=0.14, max_bin=None,\n",
       "                               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                               max_delta_step=None, max_depth=8,\n",
       "                               max_leaves=None, min_child_weight=None,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1,\n",
       "                               num_parallel_tree=None, predictor=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1, remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;author name process&#x27;,\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                                  [0]),\n",
       "                                                 (&#x27;topic name process&#x27;,\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                                  [1])])),\n",
       "                (&#x27;...\n",
       "                               feature_types=None, gamma=1, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=None,\n",
       "                               interaction_constraints=None, lambda=2.5,\n",
       "                               learning_rate=0.14, max_bin=None,\n",
       "                               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                               max_delta_step=None, max_depth=8,\n",
       "                               max_leaves=None, min_child_weight=None,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1,\n",
       "                               num_parallel_tree=None, predictor=None, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">vect: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1, remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;author name process&#x27;,\n",
       "                                 TfidfVectorizer(lowercase=False,\n",
       "                                                 token_pattern=None,\n",
       "                                                 tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                 [0]),\n",
       "                                (&#x27;topic name process&#x27;,\n",
       "                                 TfidfVectorizer(lowercase=False,\n",
       "                                                 token_pattern=None,\n",
       "                                                 tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                 [1])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">author name process</label><div class=\"sk-toggleable__content\"><pre>[0]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False, token_pattern=None,\n",
       "                tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">topic name process</label><div class=\"sk-toggleable__content\"><pre>[1]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False, token_pattern=None,\n",
       "                tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[2, 3, 4, 5, 6, 7, 8, 9]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\" ><label for=\"sk-estimator-id-80\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\" ><label for=\"sk-estimator-id-81\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=1, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, lambda=2.5, learning_rate=0.14,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
       "              predictor=None, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                   transformers=[('author name process',\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=<function word_stemming at 0x7efddf317880>),\n",
       "                                                  [0]),\n",
       "                                                 ('topic name process',\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=<function word_stemming at 0x7efddf317880>),\n",
       "                                                  [1])])),\n",
       "                ('...\n",
       "                               feature_types=None, gamma=1, gpu_id=None,\n",
       "                               grow_policy=None, importance_type=None,\n",
       "                               interaction_constraints=None, lambda=2.5,\n",
       "                               learning_rate=0.14, max_bin=None,\n",
       "                               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                               max_delta_step=None, max_depth=8,\n",
       "                               max_leaves=None, min_child_weight=None,\n",
       "                               missing=nan, monotone_constraints=None,\n",
       "                               n_estimators=100, n_jobs=-1,\n",
       "                               num_parallel_tree=None, predictor=None, ...))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_lgbm = {\n",
    "    'gamma' : 1,\n",
    "    'lambda' : 2.5,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate' : 0.14,\n",
    "    'n_jobs' : -1\n",
    "}\n",
    "\n",
    "# '**' 是一種解包（unpacking）操作符，它可以用於將字典中的鍵值對以關鍵字參數的方式傳遞給函數或方法\n",
    "xgboost = Pipeline([('vect', text_transformer),\n",
    "                    ('clf', XGBClassifier(**param_grid_lgbm))])\n",
    "\n",
    "training(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2-1. Grid sizing for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.001, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.001\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'learning_rate' : [0.005, 0.006, 0.007, 0.008, 0.009 , 0.0095, 0.0098, 0.01, 0.011, 0.012, 0.013, 0.014, 0.015], \n",
    "    'n_estimators' : [300, 310, 320, 330, 340, 350, 360, 370, 375, 380, 385, 390, 400, 420, 440],\n",
    "    'min_sum_hessian_in_leaf': [0.001, 0.01, 0.1, 1],\n",
    "    'objective' : ['regression', 'regression_l1', 'poisson']\n",
    "}\n",
    "\n",
    "best_lgbm_param, best_lgbm = grid_search_cv(text_transformer, LGBMClassifier(n_jobs=-1), param_grid_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2-2. Training for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.66793 (+/-0.00239)\n",
      "valid score: 0.59754 (+/-0.00735)\n",
      "train score: 0.67509\n",
      "valid score: 0.59149\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1, remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;author name process&#x27;,\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                                  [0]),\n",
       "                                                 (&#x27;topic name process&#x27;,\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                                  [1])])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 LGBMClassifier(learning_rate=0.009, n_estimators=380,\n",
       "                                objective=&#x27;poisson&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-82\" type=\"checkbox\" ><label for=\"sk-estimator-id-82\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 ColumnTransformer(n_jobs=-1, remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;author name process&#x27;,\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                                  [0]),\n",
       "                                                 (&#x27;topic name process&#x27;,\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                                  [1])])),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 LGBMClassifier(learning_rate=0.009, n_estimators=380,\n",
       "                                objective=&#x27;poisson&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-83\" type=\"checkbox\" ><label for=\"sk-estimator-id-83\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">vect: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(n_jobs=-1, remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;author name process&#x27;,\n",
       "                                 TfidfVectorizer(lowercase=False,\n",
       "                                                 token_pattern=None,\n",
       "                                                 tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                 [0]),\n",
       "                                (&#x27;topic name process&#x27;,\n",
       "                                 TfidfVectorizer(lowercase=False,\n",
       "                                                 token_pattern=None,\n",
       "                                                 tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;),\n",
       "                                 [1])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-84\" type=\"checkbox\" ><label for=\"sk-estimator-id-84\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">author name process</label><div class=\"sk-toggleable__content\"><pre>[0]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-85\" type=\"checkbox\" ><label for=\"sk-estimator-id-85\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False, token_pattern=None,\n",
       "                tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-86\" type=\"checkbox\" ><label for=\"sk-estimator-id-86\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">topic name process</label><div class=\"sk-toggleable__content\"><pre>[1]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-87\" type=\"checkbox\" ><label for=\"sk-estimator-id-87\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False, token_pattern=None,\n",
       "                tokenizer=&lt;function word_stemming at 0x7efddf317880&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-88\" type=\"checkbox\" ><label for=\"sk-estimator-id-88\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[2, 3, 4, 5, 6, 7, 8, 9]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-89\" type=\"checkbox\" ><label for=\"sk-estimator-id-89\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-90\" type=\"checkbox\" ><label for=\"sk-estimator-id-90\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.009, n_estimators=380, objective=&#x27;poisson&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                   transformers=[('author name process',\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=<function word_stemming at 0x7efddf317880>),\n",
       "                                                  [0]),\n",
       "                                                 ('topic name process',\n",
       "                                                  TfidfVectorizer(lowercase=False,\n",
       "                                                                  token_pattern=None,\n",
       "                                                                  tokenizer=<function word_stemming at 0x7efddf317880>),\n",
       "                                                  [1])])),\n",
       "                ('clf',\n",
       "                 LGBMClassifier(learning_rate=0.009, n_estimators=380,\n",
       "                                objective='poisson'))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params_LGBM = {\n",
    "    'random_state': 0, \n",
    "    'learning_rate' : 0.005, \n",
    "    'n_estimators' : 400,\n",
    "    'n_jobs' : -1\n",
    "}\n",
    "\n",
    "lgbm = Pipeline([('vect', text_transformer),\n",
    "                 ('clf', LGBMClassifier(**best_lgbm_param))])\n",
    "\n",
    "training(lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6. VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.77322 (+/-0.00277)\n",
      "valid score: 0.59540 (+/-0.00846)\n",
      "train score: 0.78001\n",
      "valid score: 0.59086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "prarms_voting = {\n",
    "    'estimators' : [('xgboost', xgboost), ('lgbm', lgbm)], \n",
    "    'voting' : 'soft',\n",
    "    'weights' : [0.5, 0.5],\n",
    "    'n_jobs' : -1,\n",
    "    'flatten_transform' : True,\n",
    "    'verbose' : True\n",
    "}\n",
    "\n",
    "'''\n",
    "votingClassifier = Pipeline([('vect', text_transformer),\n",
    "                 ('clf', LGBMClassifier(**prarms_voting))])\n",
    "'''\n",
    "\n",
    "\n",
    "voting = VotingClassifier([('xgboost', xgboost), ('lgbm', lgbm)],\n",
    "                          voting='soft', weights=[0.5, 0.5])\n",
    "voting = training(voting)\n",
    "\n",
    "# training(votingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array length 27643 does not match index length 11847",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text Feature Engineering.ipynb Cell 47\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m best_model \u001b[39m=\u001b[39m voting\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m y_score \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39mpredict_proba(X_test)[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m df_pred \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mId\u001b[39;49m\u001b[39m'\u001b[39;49m: test_data[\u001b[39m'\u001b[39;49m\u001b[39mId\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39mPopularity\u001b[39;49m\u001b[39m'\u001b[39;49m: y_score})\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249433231227d/users/student/mr111/mfhsieh22/NTHU_Course/Deep-Learning/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity/src/Text%20Feature%20Engineering.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m df_pred\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mtest_pred.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    737\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/users/student/mr111/mfhsieh22/miniconda3/envs/DL/lib/python3.11/site-packages/pandas/core/internals/construction.py:690\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[39mif\u001b[39;00m lengths[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m    686\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    687\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39marray length \u001b[39m\u001b[39m{\u001b[39;00mlengths[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m does not match index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlength \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m         )\n\u001b[0;32m--> 690\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    691\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     index \u001b[39m=\u001b[39m default_index(lengths[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: array length 27643 does not match index length 11847"
     ]
    }
   ],
   "source": [
    "best_model = voting\n",
    "\n",
    "y_score = best_model.predict_proba(X_test)[:, 1]\n",
    "df_pred = pd.DataFrame({'Id': test_data['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
