{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>DataLab Cup 1: Text Feature Engineering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. To load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../dataset/train.csv')\n",
    "test_data  = pd.read_csv('../dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11847, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
       "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
       "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
       "4   4          -1  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. To extract the features from the dataset\n",
    "\n",
    "將一些我們想要用到的feature從dataset中提取出來。以下為提取的特徵:\n",
    "\n",
    "- title\n",
    "- time(year/month/day/hour/minute/second)\n",
    "- number of images (num_img)\n",
    "- number of videos (num_video)\n",
    "- author name\n",
    "- topic\n",
    "- channel\n",
    "- length of content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "* beautiful soup\n",
    "    - conda install -c conda-forge beautifulsoup4\n",
    "    \n",
    "<br>\n",
    "\n",
    "* vadersentiment\n",
    "    - conda install -c conda-forge vadersentiment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# to get the attribute of the 'title', 'year/month/date/day/hour/minute/second/is_weekend', 'num_img', 'num_video', 'author name', 'topic', 'channel', 'content length', 'title_sentiment'\n",
    "\n",
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # 1. to find the 'title' (body > h1)\n",
    "    title = soup.find('h1').string.strip().lower()\n",
    "\n",
    "    # 2. to find time(body > div > span > time)\n",
    "\n",
    "    \n",
    "    date_string = soup.find('time')\n",
    "    try:\n",
    "        date_string = date_string['datetime']\n",
    "    except:\n",
    "        date_string = 'wed, 10 oct 2014 15:00:43 +0000'\n",
    "        \n",
    "    date_string = date_string.strip().lower()\n",
    "    datetimes = datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S %z')\n",
    "    \n",
    "    \n",
    "    year = datetimes.year\n",
    "    month = datetimes.month\n",
    "    date = datetimes.day\n",
    "    day = pd.Timestamp(str(year)+'-'+str(month)+'-'+str(date)).dayofweek+1\n",
    "    is_weekend = 1 if (day==6 or day==7) else 0\n",
    "    hour = datetimes.hour\n",
    "    minute = datetimes.minute\n",
    "    second = datetimes.second\n",
    "    \n",
    "\n",
    "    # 3. to find the number of images\n",
    "    num_img  = len(soup.find_all('img'))\n",
    "    \n",
    "\n",
    "    # 4. to find the number of videos\n",
    "    num_video = len(soup.find_all('iframe'))\n",
    "    \n",
    "\n",
    "    # 5. to find the author name\n",
    "    article_info = soup.find('div', class_='article-info')\n",
    "    author = article_info.find('span', class_='author_name') or article_info.find('span', class_='byline basic')\n",
    "\n",
    "    if (author != None):\n",
    "        if (author.find('a') != None):\n",
    "            author = author.find('a')\n",
    "            author_name = author.get_text().lower()\n",
    "        else :\n",
    "            author_name = author.get_text().lower()\n",
    "    else :\n",
    "        author_name = 'not found'\n",
    "    \n",
    "    \n",
    "\n",
    "    # 6. to find the article topic\n",
    "    footer = soup.find('footer', class_='article-topics')\n",
    "    topic = footer.get_text().split(': ')[1]\n",
    "    \n",
    "    # 7. to find the channel\n",
    "    channel = soup.find('article')['data-channel'].strip().lower()\n",
    "    \n",
    "    '''\n",
    "    print('title = ', title, type(title))\n",
    "    print('time = ', year, \"/\", month, \"/\",day, \" \",hour, \":\",minute, \":\",second, type(year))\n",
    "    print('number of images = ', num_img, type(num_img))\n",
    "    print('number of videos = ', num_video, type(num_video))\n",
    "    print('author_name = ', author_name, type(author_name))\n",
    "    print('topic = ', topic, type(topic))\n",
    "    print('channel = ', channel, type(channel))\n",
    "    '''\n",
    "    \n",
    "    # 8. to find the content length\n",
    "    content = soup.body.find('section', class_='article-content').get_text()\n",
    "    len_content = len(content)\n",
    "\n",
    "    # print('topic = ', topic, type(topic))\n",
    "    \n",
    "    # 9. to find the sentiment of title\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    title_sentiment = analyzer.polarity_scores(topic)\n",
    "    sentiment_neg = title_sentiment['neg']\n",
    "    sentiment_neu = title_sentiment['neu']\n",
    "    sentiment_pos = title_sentiment['pos']\n",
    "    sentiment_compound = title_sentiment['compound']\n",
    "\n",
    "    return title, author_name, channel, topic, year, month, date, day, is_weekend, hour, minute, second, num_img, num_video, len_content, sentiment_neg, sentiment_neu, sentiment_pos, sentiment_compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_list = []\n",
    "feature_test_list = []\n",
    "\n",
    "for content in (train_data['Page content']):\n",
    "    feature_train_list.append(preprocessor(content))\n",
    "for content in (test_data['Page content']):\n",
    "    feature_train_list.append(preprocessor(content))\n",
    "\n",
    "df_all = pd.DataFrame(\n",
    "        feature_train_list, \n",
    "        columns=['title', 'author_name', 'channel', 'topic', 'year', 'month', 'date', 'day', 'is_weekend', 'hour', 'minute', 'second', 'num_img', 'num_video', 'len_content', 'sentiment_neg', 'sentiment_neu', 'sentiment_pos', 'sentiment_compound'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 . Preprocessing - tokenization\n",
    "\n",
    "To split the text corpora into individual elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 . Preprocessing - Word Stemming\n",
    "\n",
    "There are two ways of word stemming\n",
    "\n",
    "1. PorterStemmer(Stemming): break the word in rule-besed way, which will lead to the probelm of overstemming\n",
    "\n",
    "2. WordNetLemmatizer(Lemmatization): Stem the words will better performance, while time-consuming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /users/student/mr111//mfhsieh22/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /users/student/mr111//mfhsieh22/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university,', 'universal,', 'university']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.data.path.append('/home/mfhsieh/NTHU-Deep-Learning-Competition/2023_DataLab_Cup1-Predicting_News_Popularity')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def word_stemming(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    lm = WordNetLemmatizer()\n",
    "    words = re.split('\\s', text.strip())\n",
    "    lemmatized_words = [lm.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "print(word_stemming('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university,', 'universal,', 'univers']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "print(tokenizer_stem('university, universal, universities'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 Preprocessing - Stop-Word Removal\n",
    "\n",
    "儘管刪除停用詞在某些情況下（例如 BoW 和特徵哈希）可以有益於簡化表示，並可能提高文字分析的準確性，但並不總是必要，特別是在使用 TF-IDF 時。是否刪除停用詞應基於文本分析任務的具體要求以及資料集的特性來進行決策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/student/mr111//mfhsieh22/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def stop_word_removal(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(stop_word_removal('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5 Preprocessing - Word Stemming + Stop-Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_word_remove_stopword(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    lm = WordNetLemmatizer()\n",
    "    words = re.split('\\s', text.strip())\n",
    "    lemmatized_words = [lm.lemmatize(word) for word in words]\n",
    "    \n",
    "    filtered_list = [word for word in lemmatized_words if word not in stop]\n",
    "    \n",
    "    return filtered_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-5 Create TF-IDF feature representation ([ref](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ngram_range_ = (1,1)\n",
    "\n",
    "text_transformer = ColumnTransformer(\n",
    "    [\n",
    "        #('title preprocess', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [0]),\n",
    "        ('author name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=ngram_range_, lowercase=False), [0]),\n",
    "        #('channel process', TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False), [1]),\n",
    "        ('topic name process', TfidfVectorizer(tokenizer=word_stemming, token_pattern=None, ngram_range=ngram_range_, lowercase=False), [1]),\n",
    "    ],\n",
    "    remainder='passthrough', # do not touch the remaining data\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "to: 2.52\n",
      "the: 2.58\n",
      "in: 2.96\n",
      "a: 3.03\n",
      "of: 3.07\n",
      "for: 3.10\n",
      "and: 3.44\n",
      "is: 3.51\n",
      "on: 3.54\n",
      "your: 3.60\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "the: 1142.4830180213792\n",
      "to: 1109.6698985304176\n",
      "a: 795.7866740412087\n",
      "in: 787.5419357082401\n",
      "of: 746.7665375362841\n",
      "for: 734.8807138633431\n",
      "and: 555.5640584744767\n",
      "your: 551.5848064261177\n",
      "is: 544.621163447621\n",
      "you: 533.4917456688497\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=word_stemming, ngram_range=(1,1), lowercase=False)\n",
    "tfidf.fit(df_all['title'])\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names_out()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(df_all['title']).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones((1, tfidf_sum.shape[0])))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model training\n",
    "\n",
    "- XGBoost\n",
    "\n",
    "- LightGBM\n",
    "\n",
    "- CatBoost\n",
    "\n",
    "- AdaBoost\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "- VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>topic</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour</th>\n",
       "      <th>num_video</th>\n",
       "      <th>len_content</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>sentiment_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clara moskowitz</td>\n",
       "      <td>Asteroid, Asteroids, challenge, Earth, Space, ...</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3591</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>christina warren</td>\n",
       "      <td>Apps and Software, Google, open source, opn pl...</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1843</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>Entertainment, NFL, NFL Draft, Sports, Televis...</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>6646</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam laird</td>\n",
       "      <td>Sports, Video, Videos, Watercooler</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1821</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connor finnegan</td>\n",
       "      <td>Entertainment, instagram, instagram video, NFL...</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8919</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_name                                              topic  year  \\\n",
       "0   clara moskowitz  Asteroid, Asteroids, challenge, Earth, Space, ...  2013   \n",
       "1  christina warren  Apps and Software, Google, open source, opn pl...  2013   \n",
       "2         sam laird  Entertainment, NFL, NFL Draft, Sports, Televis...  2014   \n",
       "3         sam laird                Sports, Video, Videos, Watercooler   2013   \n",
       "4   connor finnegan  Entertainment, instagram, instagram video, NFL...  2014   \n",
       "\n",
       "   month  date  day  is_weekend  hour  num_video  len_content  sentiment_neg  \\\n",
       "0      6    19    3           0    15          0         3591          0.000   \n",
       "1      3    28    4           0    17          0         1843          0.119   \n",
       "2      5     7    3           0    19         25         6646          0.000   \n",
       "3     10    11    5           0     2         21         1821          0.000   \n",
       "4      4    17    4           0     3          1         8919          0.000   \n",
       "\n",
       "   sentiment_neu  sentiment_pos  sentiment_compound  \n",
       "0          0.822          0.178              0.0772  \n",
       "1          0.881          0.000             -0.2263  \n",
       "2          0.641          0.359              0.4215  \n",
       "3          1.000          0.000              0.0000  \n",
       "4          0.641          0.359              0.4215  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_dict = [  #'title', \n",
    "                    'author_name', \n",
    "                    #'channel', \n",
    "                    'topic', \n",
    "                    'year', \n",
    "                    'month',\n",
    "                    'date', \n",
    "                    'day', \n",
    "                    'is_weekend',\n",
    "                    'hour', \n",
    "                    # 'minute', \n",
    "                    # 'second', \n",
    "                    # 'num_img', \n",
    "                    'num_video', \n",
    "                    'len_content',\n",
    "                    'sentiment_neg', \n",
    "                    'sentiment_neu', \n",
    "                    'sentiment_pos', \n",
    "                    'sentiment_compound'\n",
    "                    ]\n",
    "\n",
    "df = df_all.loc[:, remaining_dict]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 14)\n",
      "<class 'numpy.ndarray'>\n",
      "[['clara moskowitz'\n",
      "  'Asteroid, Asteroids, challenge, Earth, Space, U.S., World ' 2013 ...\n",
      "  0.822 0.178 0.0772]\n",
      " ['christina warren'\n",
      "  'Apps and Software, Google, open source, opn pledge, patent lawsuit theater, software patents, Tech, U.S. '\n",
      "  2013 ... 0.881 0.0 -0.2263]\n",
      " ['sam laird' 'Entertainment, NFL, NFL Draft, Sports, Television ' 2014\n",
      "  ... 0.641 0.359 0.4215]\n",
      " ...\n",
      " ['christine erickson' 'Food, hot dogs, humor, Photography, Watercooler '\n",
      "  2014 ... 0.704 0.296 0.2732]\n",
      " ['seth fiegerman' 'Business, marissa mayer, Media, stocks, Yahoo ' 2013\n",
      "  ... 1.0 0.0 0.0]\n",
      " ['megan ranney' 'austin, Business, CurioCity, Small Business, Startups '\n",
      "  2014 ... 1.0 0.0 0.0]]\n",
      "(27643,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = df.values[:train_data.shape[0]]\n",
    "y_train = train_data['Popularity'].values\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = df.values[train_data.shape[0]:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(type(X_train))\n",
    "print(X_train)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "print(y_train)\n",
    "\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To construct the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def training(clf):\n",
    "    clf_cv = cross_validate(clf, X_train, y_train, scoring='roc_auc', return_train_score=True, return_estimator=True)\n",
    "    print('train score: {:.6f} (+/-{:.6f})'.format(\n",
    "        np.mean(clf_cv['train_score']), np.std(clf_cv['train_score'])))\n",
    "    print('valid score: {:.6f} (+/-{:.6f})'.format(\n",
    "        np.mean(clf_cv['test_score']), np.std(clf_cv['test_score'])))\n",
    "\n",
    "    train_score =  np.mean(clf_cv['train_score'])\n",
    "    valid_score = np.mean(clf_cv['test_score'])\n",
    "\n",
    "    clf.fit(X_train_split, y_train_split)\n",
    "    # print('train score: {:.5f}'.format(roc_auc_score(\n",
    "    #     y_train_split, clf.predict_proba(X_train_split)[:, 1])))\n",
    "    # print('valid score: {:.5f}'.format(roc_auc_score(\n",
    "    #     y_valid_split, clf.predict_proba(X_valid_split)[:, 1])))\n",
    "    \n",
    "    return clf_cv, train_score, valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To contruct the grid search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_cv(ct, clf, param_grid, verbose_=False):\n",
    "    X_train_ct = ct.fit_transform(X_train)\n",
    "    \n",
    "    # to report the grid search information\n",
    "    if(verbose_):\n",
    "        gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=-1, cv=5, return_train_score=True, verbose = 2)\n",
    "    else:\n",
    "        gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='roc_auc', n_jobs=-1, cv=5, return_train_score=True)\n",
    "    gs.fit(X_train_ct, y_train)\n",
    "    \n",
    "    results, idx = gs.cv_results_, gs.best_index_\n",
    "    print('train score: {:.6f} (+/-{:.6f})'.format(results['mean_train_score'][idx], results['std_train_score'][idx]))\n",
    "    print('valid score: {:.6f} (+/-{:.6f})'.format(results['mean_test_score'][idx], results['std_test_score'][idx]))\n",
    "    print('best params:', gs.best_params_)\n",
    "    return gs.best_params_, gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **- To set whether to run the grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_en = True\n",
    "grid_search_en = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - to store the best parameter to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_storage(dict_path, file_name, best_xgb_param):\n",
    "    if not os.path.exists(dict_path):\n",
    "        os.makedirs(dict_path)\n",
    "        \n",
    "    file_path = os.path.join(dict_path, file_name + \".txt\")\n",
    "    \n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(f'The features: {remaining_dict}\\n')\n",
    "        file.write(f'The best parameter: {best_xgb_param}\\n')\n",
    "        file.write(f'ngram_range_: {ngram_range_}')\n",
    "        file.write('--------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-1-1. Grid sizing for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid_xgb = {\n",
    "#     'gamma' : [0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
    "#     'lambda' : [2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3],\n",
    "#     'n_estimators': [80, 85, 90, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 110],\n",
    "#     'max_depth': [6, 7, 8, 9, 10],\n",
    "#     'learning_rate' : [0.13, 0.131, 0.132, 0.133, 0.134, 0.135 ,0.136, 0.137, 0.138, 0.139, 0.14, 0.141, 0.142, 0.143, 0.144, 0.145, 0.146, 0.147, 0.148, 0.149, 0.15]  \n",
    "# }\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'lambda' : [2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3]  \n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_xgb_param, best_xgb = grid_search_cv(text_transformer, XGBClassifier(n_jobs=-1), param_grid_xgb, True)\n",
    "    parameter_storage('../output/best_parameters', 'best_xgb_param', best_xgb_param)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-1-2. Training for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.81595 (+/-0.00300)\n",
      "valid score: 0.58880 (+/-0.01144)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'fit_time': array([7.31572628, 4.1077981 , 4.77429843, 4.59873605, 5.02125406]),\n",
       "  'score_time': array([1.54502249, 1.62641263, 1.62887645, 1.49892545, 1.61216593]),\n",
       "  'estimator': [Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('...\n",
       "                                  feature_types=None, gamma=1, gpu_id=None,\n",
       "                                  grow_policy=None, importance_type=None,\n",
       "                                  interaction_constraints=None, lambda=2.5,\n",
       "                                  learning_rate=0.14, max_bin=None,\n",
       "                                  max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                                  max_delta_step=None, max_depth=8,\n",
       "                                  max_leaves=None, min_child_weight=None,\n",
       "                                  missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=100, n_jobs=-1,\n",
       "                                  num_parallel_tree=None, predictor=None, ...))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('...\n",
       "                                  feature_types=None, gamma=1, gpu_id=None,\n",
       "                                  grow_policy=None, importance_type=None,\n",
       "                                  interaction_constraints=None, lambda=2.5,\n",
       "                                  learning_rate=0.14, max_bin=None,\n",
       "                                  max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                                  max_delta_step=None, max_depth=8,\n",
       "                                  max_leaves=None, min_child_weight=None,\n",
       "                                  missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=100, n_jobs=-1,\n",
       "                                  num_parallel_tree=None, predictor=None, ...))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('...\n",
       "                                  feature_types=None, gamma=1, gpu_id=None,\n",
       "                                  grow_policy=None, importance_type=None,\n",
       "                                  interaction_constraints=None, lambda=2.5,\n",
       "                                  learning_rate=0.14, max_bin=None,\n",
       "                                  max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                                  max_delta_step=None, max_depth=8,\n",
       "                                  max_leaves=None, min_child_weight=None,\n",
       "                                  missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=100, n_jobs=-1,\n",
       "                                  num_parallel_tree=None, predictor=None, ...))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('...\n",
       "                                  feature_types=None, gamma=1, gpu_id=None,\n",
       "                                  grow_policy=None, importance_type=None,\n",
       "                                  interaction_constraints=None, lambda=2.5,\n",
       "                                  learning_rate=0.14, max_bin=None,\n",
       "                                  max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                                  max_delta_step=None, max_depth=8,\n",
       "                                  max_leaves=None, min_child_weight=None,\n",
       "                                  missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=100, n_jobs=-1,\n",
       "                                  num_parallel_tree=None, predictor=None, ...))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('...\n",
       "                                  feature_types=None, gamma=1, gpu_id=None,\n",
       "                                  grow_policy=None, importance_type=None,\n",
       "                                  interaction_constraints=None, lambda=2.5,\n",
       "                                  learning_rate=0.14, max_bin=None,\n",
       "                                  max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                                  max_delta_step=None, max_depth=8,\n",
       "                                  max_leaves=None, min_child_weight=None,\n",
       "                                  missing=nan, monotone_constraints=None,\n",
       "                                  n_estimators=100, n_jobs=-1,\n",
       "                                  num_parallel_tree=None, predictor=None, ...))])],\n",
       "  'test_score': array([0.58999162, 0.58737407, 0.59788165, 0.56809562, 0.60063559]),\n",
       "  'train_score': array([0.81060466, 0.81730845, 0.81918499, 0.81770501, 0.81492581])},\n",
       " 0.815945783770785,\n",
       " 0.5887957112778638)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'gamma' : 1,\n",
    "    'lambda' : 2.5,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate' : 0.14,\n",
    "    'n_jobs' : -1\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    xgboost = Pipeline([('vect', text_transformer), ('clf', best_xgb)])\n",
    "else :\n",
    "    xgboost = Pipeline([('vect', text_transformer), ('clf', XGBClassifier(**param_grid_lgbm))])\n",
    "    \n",
    "training(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2-1. Grid sizing for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lgbm = {\n",
    "    'learning_rate' : [0.013, 0.0135 , 0.0136, 0.0137, 0.0138, 0.0139, 0.014, 0.0141, 0.0142, 0.0143, 0.0144 ,0.0145, 0.015], \n",
    "    'n_estimators' : [230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250],\n",
    "    'objective' : ['regression', 'regression_l1', 'poisson']\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_lgbm_param, best_lgbm = grid_search_cv(text_transformer, LGBMClassifier(n_jobs=-1, verbose=-1), param_grid_lgbm, True)\n",
    "    parameter_storage('../output/best_parameters', 'best_lgbm_param', best_lgbm_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2-2. Training for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.66906 (+/-0.00230)\n",
      "valid score: 0.59698 (+/-0.00746)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'fit_time': array([7.78950667, 7.81372881, 5.65546751, 6.22436523, 4.83242774]),\n",
       "  'score_time': array([1.8156426 , 1.60529828, 1.25823498, 1.27095294, 1.22990751]),\n",
       "  'estimator': [Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    LGBMClassifier(learning_rate=0.014, n_estimators=240,\n",
       "                                   objective='poisson', random_state=0))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    LGBMClassifier(learning_rate=0.014, n_estimators=240,\n",
       "                                   objective='poisson', random_state=0))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    LGBMClassifier(learning_rate=0.014, n_estimators=240,\n",
       "                                   objective='poisson', random_state=0))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    LGBMClassifier(learning_rate=0.014, n_estimators=240,\n",
       "                                   objective='poisson', random_state=0))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    LGBMClassifier(learning_rate=0.014, n_estimators=240,\n",
       "                                   objective='poisson', random_state=0))])],\n",
       "  'test_score': array([0.60010472, 0.5954672 , 0.59862134, 0.58397412, 0.60671303]),\n",
       "  'train_score': array([0.668947  , 0.67207916, 0.67041962, 0.66873784, 0.66513463])},\n",
       " 0.6690636465491954,\n",
       " 0.5969760824449507)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_LGBM = {\n",
    "    'random_state': 0, \n",
    "    'learning_rate' : 0.014,\n",
    "    'n_estimators' : 240,\n",
    "    'n_jobs' : -1,\n",
    "    'objective' : 'poisson'\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    lgbm = Pipeline([('vect', text_transformer), ('clf', best_lgbm)])\n",
    "else :\n",
    "    lgbm = Pipeline([('vect', text_transformer), ('clf', LGBMClassifier(**params_LGBM))])\n",
    "\n",
    "training(lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-3-1. Grid sizing for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_catboost = {\n",
    "    'learning_rate' : [0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.02, 0.03], \n",
    "    'n_estimators' : [450, 500, 550],\n",
    "    'depth' : [9, 10, 11]\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_cat_params, best_cat_estimator = grid_search_cv(text_transformer, CatBoostClassifier(eval_metric='AUC',random_state=0, verbose=False), param_grid_catboost, True)\n",
    "    parameter_storage('../output/best_parameters', 'best_cat_params', best_cat_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-3-2. Training for CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.69435 (+/-0.00296)\n",
      "valid score: 0.59772 (+/-0.00945)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'fit_time': array([76.30919909, 75.94720244, 76.99515271, 78.02176642, 76.28989005]),\n",
       "  'score_time': array([1.37266779, 1.22673154, 2.68824267, 1.75454402, 0.22225666]),\n",
       "  'estimator': [Pipeline(steps=[('ct',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    <catboost.core.CatBoostClassifier object at 0x7febe410b290>)]),\n",
       "   Pipeline(steps=[('ct',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    <catboost.core.CatBoostClassifier object at 0x7febe410b2d0>)]),\n",
       "   Pipeline(steps=[('ct',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    <catboost.core.CatBoostClassifier object at 0x7fec5c6e8690>)]),\n",
       "   Pipeline(steps=[('ct',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    <catboost.core.CatBoostClassifier object at 0x7fec140d71d0>)]),\n",
       "   Pipeline(steps=[('ct',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    <catboost.core.CatBoostClassifier object at 0x7fec7f192d90>)])],\n",
       "  'test_score': array([0.6008011 , 0.59536158, 0.60437722, 0.5806153 , 0.60745901]),\n",
       "  'train_score': array([0.69060871, 0.69594192, 0.69880614, 0.69471434, 0.69166332])},\n",
       " 0.6943468861967415,\n",
       " 0.5977228441288256)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsCatBoost = {\n",
    "    'eval_metric' : 'AUC',\n",
    "    'n_estimators' : 500,\n",
    "    'depth' : 10,\n",
    "    'learning_rate' : 0.01,\n",
    "    'random_state' : 0,\n",
    "    'verbose' : False\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    catboost = Pipeline([('ct', text_transformer),('clf', best_cat_estimator)])\n",
    "else :\n",
    "    catboost = Pipeline([('ct', text_transformer),('clf', CatBoostClassifier(**paramsCatBoost))])\n",
    "\n",
    "training(catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-4-1. Grid sizing for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_grid_AdaBoost = {\n",
    "    'clf__estimator__max_depth' : [1,2,3,4,5,6,7,8,9,10],\n",
    "    'clf__n_estimators': [num for num in range(50, 1000, 50)],\n",
    "    'clf__learning_rate': [0.005, 0.01, 0.05, 0.1, 0.5] \n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_ada_params, best_ada_estimator = grid_search_cv(text_transformer, \n",
    "                                                         AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_jobs=-1), \n",
    "                                                         params_grid_AdaBoost, True)\n",
    "    parameter_storage('../output/best_parameters', 'best_ada_params', best_ada_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-4-2. Training for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.68188 (+/-0.00210)\n",
      "valid score: 0.58661 (+/-0.00867)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'fit_time': array([110.84930491, 111.8500824 , 110.29252172, 109.72195292,\n",
       "         112.2311964 ]),\n",
       "  'score_time': array([3.19673228, 2.97644973, 2.47645974, 2.80361414, 2.85160875]),\n",
       "  'estimator': [Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=4),\n",
       "                                       learning_rate=0.005, n_estimators=900))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=4),\n",
       "                                       learning_rate=0.005, n_estimators=900))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=4),\n",
       "                                       learning_rate=0.005, n_estimators=900))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=4),\n",
       "                                       learning_rate=0.005, n_estimators=900))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=4),\n",
       "                                       learning_rate=0.005, n_estimators=900))])],\n",
       "  'test_score': array([0.59036703, 0.5881894 , 0.58919954, 0.56998676, 0.59531592]),\n",
       "  'train_score': array([0.68015062, 0.68285457, 0.68309767, 0.68452044, 0.67876041])},\n",
       " 0.6818767397593722,\n",
       " 0.5866117304234253)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_Adaboost = {\n",
    "    'estimator' : DecisionTreeClassifier(max_depth = 4), \n",
    "    'learning_rate' : 0.005, \n",
    "    'n_estimators' : 900\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    adaBoost = Pipeline([('vect', text_transformer), ('clf', best_ada_estimator)])\n",
    "else :\n",
    "    adaBoost = Pipeline([('vect', text_transformer), ('clf', AdaBoostClassifier(**param_Adaboost))])\n",
    "    \n",
    "training(adaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-5-1. Grid sizing for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_grid_RF = {\n",
    "    'n_estimator' : {800, 900, 1000, 1100},\n",
    "    'max_depth' : {70, 80, 90}\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    best_RF_params, best_RF_estimator = grid_search_cv(text_transformer, RandomForestClassifier(n_jobs=-1), params_grid_RF, True)\n",
    "    parameter_storage('../output/best_parameters', 'best_RF_params', best_RF_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-5-2. Training for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.99794 (+/-0.00035)\n",
      "valid score: 0.58570 (+/-0.01065)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'fit_time': array([8.18055177, 6.2160027 , 6.56167364, 6.41827989, 6.03820419]),\n",
       "  'score_time': array([1.44979787, 1.3041687 , 1.31020141, 0.48848915, 1.38508534]),\n",
       "  'estimator': [Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    RandomForestClassifier(max_depth=80, n_estimators=1000,\n",
       "                                           n_jobs=-1, random_state=0))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    RandomForestClassifier(max_depth=80, n_estimators=1000,\n",
       "                                           n_jobs=-1, random_state=0))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    RandomForestClassifier(max_depth=80, n_estimators=1000,\n",
       "                                           n_jobs=-1, random_state=0))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    RandomForestClassifier(max_depth=80, n_estimators=1000,\n",
       "                                           n_jobs=-1, random_state=0))]),\n",
       "   Pipeline(steps=[('vect',\n",
       "                    ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                                      transformers=[('author name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [0]),\n",
       "                                                    ('topic name process',\n",
       "                                                     TfidfVectorizer(lowercase=False,\n",
       "                                                                     token_pattern=None,\n",
       "                                                                     tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                     [1])])),\n",
       "                   ('clf',\n",
       "                    RandomForestClassifier(max_depth=80, n_estimators=1000,\n",
       "                                           n_jobs=-1, random_state=0))])],\n",
       "  'test_score': array([0.59086455, 0.58712083, 0.58964405, 0.56514422, 0.59574219]),\n",
       "  'train_score': array([0.99765897, 0.99792258, 0.99829997, 0.99834752, 0.99746393])},\n",
       " 0.9979385918047867,\n",
       " 0.5857031689462546)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_RF = {\n",
    "    'n_jobs' : -1,\n",
    "    'random_state' : 0,\n",
    "    'n_estimators' : 1000,\n",
    "    'max_depth' : 80\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    RF = Pipeline([('vect', text_transformer), ('clf', best_RF_estimator)])\n",
    "else :\n",
    "    RF = Pipeline([('vect', text_transformer), ('clf', RandomForestClassifier(**param_RF))])\n",
    "    \n",
    "training(RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6. VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2-1. Grid sizing for Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classifier = 5\n",
    "weight_range = 2\n",
    "# estimator_list = [('xgboost', xgboost), ('lgbm', lgbm)]\n",
    "estimator_list = [('xgboost', xgboost), ('lgbm', lgbm), ('catboost', catboost), ('adaBoost', adaBoost), ('RF', RF)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(estimator_list) != num_classifier) :\n",
    "    print(\"Error: the numver of the classifier must equal to the estimator_list element number! Please check again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to find the weight combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def weight_list_generator(weight_range_, num_classifier_):   \n",
    "    weight_range = weight_range_\n",
    "    weight_list = []\n",
    "\n",
    "    binary_values = (i for i in range(1, weight_range+1))\n",
    "    weight_list = [list(i) for i in list(itertools.product(binary_values, repeat=num_classifier_))]\n",
    "\n",
    "    print('weight list = ', weight_list)\n",
    "    print('length of weight list = ', len(weight_list))\n",
    "    return weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight list =  [[1, 1], [1, 2], [2, 1], [2, 2]]\n",
      "length of weight list =  4\n"
     ]
    }
   ],
   "source": [
    "weight_list = weight_list_generator(weight_range_ = weight_range, \n",
    "                      num_classifier_ = num_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* multi-thread grid search for voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current weight =current weight = [1, 2]\n",
      " [1, 1]\n",
      "current weight = [2, 1]\n",
      "current weight = [2, 2]\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  10.7s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  11.8s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  12.2s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  12.8s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  12.1s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  13.6s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  16.3s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  15.8s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  11.1s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  12.2s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  13.3s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  14.4s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  15.8s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  18.2s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  15.5s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  16.2s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=   9.7s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  11.1s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  11.1s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  13.4s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  13.8s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  18.4s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  19.2s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  18.9s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  12.7s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  11.4s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  12.2s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  13.2s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  16.1s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  19.6s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  24.1s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  31.8s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  14.6s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  14.9s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  12.7s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  14.4s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  20.1s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  13.7s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  27.8s\n",
      "train score: 0.752094 (+/-0.002405)\n",
      "valid score: 0.597470 (+/-0.009968)\n",
      "train score: 0.794524 (+/-0.002839)\n",
      "valid score: 0.593455 (+/-0.011067)\n",
      "train score: 0.776926 (+/-0.002658)\n",
      "valid score: 0.595621 (+/-0.010656)\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  25.6s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  12.1s\n",
      "train score: 0.776926 (+/-0.002658)\n",
      "valid score: 0.595621 (+/-0.010656)\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  13.6s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  13.6s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=  12.3s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  17.3s\n",
      "[2, 1] Finish!!\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  26.0s\n",
      "[1, 2] Finish!!\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  19.8s\n",
      "[1, 1] Finish!!\n",
      "end once\n",
      "best_valid_score = %.6f 0.5974704809644273\n",
      "best_weight =  [1, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ..................... (2 of 2) Processing lgbm, total=  15.8s\n",
      "[2, 2] Finish!!\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "param_grid_voting_static = {\n",
    "    'estimators' : estimator_list,\n",
    "    'voting' : 'soft',\n",
    "    'flatten_transform' : True, \n",
    "    'verbose' : True\n",
    "}\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_weight(weight):\n",
    "    # share in multiple threads\n",
    "    global best_valid_score, best_weight, best_voting\n",
    "\n",
    "    voting = VotingClassifier(**param_grid_voting_static, weights=weight)\n",
    "    print('current weight =', weight)\n",
    "    clf_voting, train_voting, valid_voting = training(voting)\n",
    "\n",
    "    # to protect the safety of shared variables\n",
    "    with lock:\n",
    "        if valid_voting > best_valid_score:\n",
    "            best_valid_score = valid_voting\n",
    "            best_weight = weight\n",
    "            best_voting = clf_voting\n",
    "    \n",
    "    print(f'{weight} Finish!!')\n",
    "    threadmax.release()\n",
    "\n",
    "'''\n",
    "def process_weight(weight):\n",
    "    # share in multiple threads\n",
    "    global best_valid_score, best_weight, best_voting\n",
    "\n",
    "    voting = VotingClassifier(**param_grid_voting_static, weights=weight)\n",
    "    print('current weight =', weight)\n",
    "    valid_voting = 2\n",
    "\n",
    "    # to protect the safety of shared variables\n",
    "    with lock:\n",
    "        if valid_voting >= best_valid_score:\n",
    "            best_valid_score = valid_voting\n",
    "            best_weight = weight\n",
    "    \n",
    "    print(f'{weight} Finish!!')\n",
    "    threadmax.release()\n",
    "''' \n",
    "        \n",
    "if (1):\n",
    "    best_valid_score = 0\n",
    "    best_weight = None\n",
    "    best_voting = None\n",
    "    mem = []\n",
    "    threadmax = threading.BoundedSemaphore(64)\n",
    "    \n",
    "    for weight in weight_list:\n",
    "        threadmax.acquire()\n",
    "        thread = threading.Thread(target=process_weight, args=(weight,))\n",
    "    \n",
    "        thread.start()\n",
    "        mem.append(thread)\n",
    "\n",
    "    for thread in mem:\n",
    "        thread.join()\n",
    "        mem.remove(thread)\n",
    "    \n",
    "    print('end once')\n",
    "\n",
    "    print('best_valid_score = %.6f' % best_valid_score)\n",
    "    print('best_weight = ', best_weight)\n",
    "    \n",
    "    parameter_storage('../output/best_parameters', 'best_weight', best_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# param_grid_voting = {\n",
    "#     'weight' : weight_list\n",
    "# }\n",
    "\n",
    "# param_grid_voting_static = {\n",
    "#     'estimators' : estimator_list,\n",
    "#     'voting' : 'soft',\n",
    "#     'flatten_transform' : True, \n",
    "#     'verbose' : True\n",
    "# }\n",
    "\n",
    "# print(param_grid_voting['weight'])\n",
    "\n",
    "# best_valid_score = 0\n",
    "# best_weight = []\n",
    "\n",
    "# # grid search for the weight of voting classifier\n",
    "# if (grid_search_en):\n",
    "#     for weight in param_grid_voting['weight']:\n",
    "#         voting = VotingClassifier(**param_grid_voting_static, weights=weight)\n",
    "#         clf_voting, train_voting, valid_voting = training(voting)\n",
    "#         if(valid_voting>best_valid_score):\n",
    "#             best_valid_score = valid_voting\n",
    "#             best_weight = weight\n",
    "#             best_voting = clf_voting\n",
    "\n",
    "# print('best_valid_score = ', best_valid_score)\n",
    "# print('best_weight = ', best_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] .................. (1 of 2) Processing xgboost, total=   7.1s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=   7.0s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=   4.5s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=   6.7s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=   4.6s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=   5.7s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=   3.0s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=   6.3s\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=   4.1s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=   5.8s\n",
      "train score: 0.794524 (+/-0.002839)\n",
      "valid score: 0.593455 (+/-0.011067)\n",
      "[Voting] .................. (1 of 2) Processing xgboost, total=   4.3s\n",
      "[Voting] ..................... (2 of 2) Processing lgbm, total=   5.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'fit_time': array([14.05162358, 11.2028079 , 10.30600071,  9.29445767,  9.90216517]),\n",
       "  'score_time': array([3.47999477, 3.16721869, 3.15946531, 3.25798464, 1.59837985]),\n",
       "  'estimator': [VotingClassifier(estimators=[('xgboost',\n",
       "                                 Pipeline(steps=[('vect',\n",
       "                                                  ColumnTransformer(n_jobs=-1,\n",
       "                                                                    remainder='passthrough',\n",
       "                                                                    transformers=[('author '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<fu...\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [1])])),\n",
       "                                                 ('clf',\n",
       "                                                  LGBMClassifier(learning_rate=0.014,\n",
       "                                                                 n_estimators=240,\n",
       "                                                                 objective='poisson',\n",
       "                                                                 random_state=0))]))],\n",
       "                    verbose=True, voting='soft', weights=[2, 1]),\n",
       "   VotingClassifier(estimators=[('xgboost',\n",
       "                                 Pipeline(steps=[('vect',\n",
       "                                                  ColumnTransformer(n_jobs=-1,\n",
       "                                                                    remainder='passthrough',\n",
       "                                                                    transformers=[('author '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<fu...\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [1])])),\n",
       "                                                 ('clf',\n",
       "                                                  LGBMClassifier(learning_rate=0.014,\n",
       "                                                                 n_estimators=240,\n",
       "                                                                 objective='poisson',\n",
       "                                                                 random_state=0))]))],\n",
       "                    verbose=True, voting='soft', weights=[2, 1]),\n",
       "   VotingClassifier(estimators=[('xgboost',\n",
       "                                 Pipeline(steps=[('vect',\n",
       "                                                  ColumnTransformer(n_jobs=-1,\n",
       "                                                                    remainder='passthrough',\n",
       "                                                                    transformers=[('author '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<fu...\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [1])])),\n",
       "                                                 ('clf',\n",
       "                                                  LGBMClassifier(learning_rate=0.014,\n",
       "                                                                 n_estimators=240,\n",
       "                                                                 objective='poisson',\n",
       "                                                                 random_state=0))]))],\n",
       "                    verbose=True, voting='soft', weights=[2, 1]),\n",
       "   VotingClassifier(estimators=[('xgboost',\n",
       "                                 Pipeline(steps=[('vect',\n",
       "                                                  ColumnTransformer(n_jobs=-1,\n",
       "                                                                    remainder='passthrough',\n",
       "                                                                    transformers=[('author '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<fu...\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [1])])),\n",
       "                                                 ('clf',\n",
       "                                                  LGBMClassifier(learning_rate=0.014,\n",
       "                                                                 n_estimators=240,\n",
       "                                                                 objective='poisson',\n",
       "                                                                 random_state=0))]))],\n",
       "                    verbose=True, voting='soft', weights=[2, 1]),\n",
       "   VotingClassifier(estimators=[('xgboost',\n",
       "                                 Pipeline(steps=[('vect',\n",
       "                                                  ColumnTransformer(n_jobs=-1,\n",
       "                                                                    remainder='passthrough',\n",
       "                                                                    transformers=[('author '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<fu...\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [0]),\n",
       "                                                                                  ('topic '\n",
       "                                                                                   'name '\n",
       "                                                                                   'process',\n",
       "                                                                                   TfidfVectorizer(lowercase=False,\n",
       "                                                                                                   token_pattern=None,\n",
       "                                                                                                   tokenizer=<function word_stemming at 0x7fef207d9760>),\n",
       "                                                                                   [1])])),\n",
       "                                                 ('clf',\n",
       "                                                  LGBMClassifier(learning_rate=0.014,\n",
       "                                                                 n_estimators=240,\n",
       "                                                                 objective='poisson',\n",
       "                                                                 random_state=0))]))],\n",
       "                    verbose=True, voting='soft', weights=[2, 1])],\n",
       "  'test_score': array([0.5951034 , 0.59241258, 0.6009862 , 0.57328202, 0.60549285]),\n",
       "  'train_score': array([0.78973747, 0.79565227, 0.79820471, 0.79564373, 0.79338189])},\n",
       " 0.7945240142536611,\n",
       " 0.593455409555834)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no n_jobs\n",
    "prarms_voting = {\n",
    "    'estimators' : estimator_list, \n",
    "    'voting' : 'soft',\n",
    "    'weights' : [2, 1],\n",
    "    'flatten_transform' : True,\n",
    "    'verbose' : True\n",
    "}\n",
    "\n",
    "if (grid_search_en):\n",
    "    voting = VotingClassifier(**param_grid_voting_static, weights=best_weight)\n",
    "else :\n",
    "    voting = VotingClassifier(**prarms_voting)\n",
    "\n",
    "training(voting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = voting\n",
    "\n",
    "y_score = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_pred = pd.DataFrame({'Id': test_data['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('../output/test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
